{"id": "11481172", "question": "Does the manic/mixed episode distinction in bipolar disorder patients run true over time?", "context": "The authors sought to determine whether the manic/mixed episode distinction in patients with bipolar disorder runs true over time.\n\nOver an 11-year period, the observed distribution of manic and mixed episodes (N=1,224) for patients with three or more entries in the management information system of a community mental health center (N=241) was compared to the expected distribution determined by averaging 1,000 randomly generated simulations.\n\nEpisodes were consistent (all manic or all mixed) in significantly more patients than would be expected by chance.", "target": "yes", "year": "2001", "labels": ["OBJECTIVE", "METHOD", "RESULTS"]}
{"id": "17621202", "question": "Does shaving the incision site increase the infection rate after spinal surgery?", "context": "A prospective randomized clinical study.\n\nTo determine whether shaving the incision site before spinal surgery causes postsurgical infection.\n\nSpine surgeons usually shave the skin of the incision site immediately before surgery is performed. However, evidence from some surgical series suggests that presurgical shaving may increase the postsurgical infection rate. To our knowledge, no previously published studies have addressed this issue.\n\nA total of 789 patients scheduled to undergo spinal surgery were randomly allocated into 2 groups: those in whom the site of operation was shaved immediately before surgery (shaved group; 371 patients) and the patients in whom presurgical shaving was not performed (unshaved group; 418 patients). The mean duration of anesthesia and the infection rates in both groups were recorded and compared.\n\nThe duration of anesthesia did not differ in the 2 groups (P>0.05). A postoperative infection developed in 4 patients in the shaved group and in 1 patient in the nonshaved group (P<0.01).", "target": "maybe", "year": "2007", "labels": ["STUDY DESIGN", "OBJECTIVE", "SUMMARY OF BACKGROUND DATA", "METHODS", "RESULTS"]}
{"id": "22970993", "question": "Does sex affect the outcome of laparoscopic cholecystectomy?", "context": "The aim of our study was to determine the effect of sex on the outcome of laparoscopic cholecystectomy in terms of operative time, conversion to open cholecystectomy, postoperative complications and mean hospital stay.\n\nIn this retrospective observational study, we analyzed the medical records of 2061 patients who underwent laparoscopic cholecystectomy in the surgical department of Khyber Teaching Hospital (Peshawar, Pakistan) between March 2008 and January 2010. χ(2) test and t-test were respectively used to analyze categorical and numerical variables. P ≤ 0.05 was considered significant.\n\nThe study included 1772 female and 289 male patients. The mean age for male patients was 44.07 ± 11.91 years compared to 41.29 ± 12.18 years for female patients (P = 0.706). Laparoscopic cholecystectomy was successfully completed in 1996 patients. The conversion rate was higher in men (P < 0.001), and the mean operating time was longer in men (P < 0.001). Bile duct injuries occurred more frequently in men (P < 0.001). Gallbladder perforation and gallstone spillage also occurred more commonly in men (P = 0.001); similarly severe inflammation was reported more in male patients (P = 0001). There were no statistically significant differences in mean hospital stay, wound infection and port-site herniation between men and women. Multivariate regression analysis showed that the male sex is an independent risk factor for conversion to open cholecystectomy (odds ratio = 2.65, 95% confidence interval: 1.03-6.94, P = 0.041) and biliary injuries (odds ratio = 0.95, 95% confidence interval: 0.91-0.99, P-value = 0.036).", "target": "yes", "year": "2013", "labels": ["INTRODUCTION", "METHODS", "RESULTS"]}
{"id": "26194560", "question": "Does cup-cage reconstruction with oversized cups provide initial stability in THA for osteoporotic acetabular fractures?", "context": "The incidence of acetabular fractures in osteoporotic patients is increasing. Immediate total hip arthroplasty (THA) has potential advantages, but achieving acetabular component stability is challenging and, at early followup, reported revision rates for loosening are high.QUESTIONS/\n\nThis study measured acetabular component stability and the initial surface contact achieved between the acetabular component and unfractured region of the pelvis after THA using an oversized acetabular component and cup-cage reconstruction.\n\nBetween November 2011 and November 2013, we treated 40 acute acetabular fractures in patients older than 70 years of age. Of these, 12 (30%) underwent immediate THA using an oversized acetabular component with screws inserted only into the ilium and a cup-cage construct. Postoperatively all patients were mobilized without weightbearing restrictions. Indications for immediate THA after acetabular fractures were displaced articular comminution deemed unreducible. Eleven of the 12 were prospectively studied to evaluate the initial stability of the reconstructions using radiostereometric analysis. One of the patients died of a pulmonary embolism after surgery, and the remaining 10 (median age, 81 years; range, 72-86 years) were studied. Of these, five were analyzed at 1 year and five were analyzed at 2 years. Acetabular component migration was defined as acceptable if less than the limits for primary THA that predict later loosening (1.76 mm of proximal migration and 2.53° of sagittal rotation). The contact surface between the acetabular component and ilium in direct continuity with the sacroiliac joint, and the ischium and pubis in direct continuity with the symphysis pubis, was measured on postoperative CT scans.\n\nAt 1 year the median proximal migration was 0.83 mm (range, 0.09-5.13 mm) and sagittal rotation was 1.3° (range, 0.1°-7.4°). Three of the 10 components had migration above the suggested limits for primary THA at 1 year postoperatively. The contact surface achieved at surgery between the acetabular component and pelvis ranged from 11 to 17 cm(2) (15%-27% of each component).", "target": "yes", "year": "2015", "labels": ["BACKGROUND", "PURPOSES", "METHODS", "RESULTS"]}
{"id": "11149643", "question": "Is semi-closed endarterectomy of the superficial femoral artery combined with a short venous bypass in case of insufficient venous material an acceptable alternative for limb-threatening ischemia?", "context": "The aim of this study was to analyse the results of infragenual arterial revascularisation using semiclosed endarterectomy of the superficial femoral artery combined with a short venous bypass in patients with critical leg ischemia and insufficient venous material for a straightforward femorocrural reconstruction.\n\nFrom December 1990 through December 1998 thirty patients were studied (22 males and 8 females; mean age 65 years, range 31-92 years). The mean follow-up was 26 months (range 1-96 months). Cumulative primary patency and limb salvage rates were calculated according to life-table analysis.\n\nThe cumulative primary patency was 60.3% at 1 year and 48.4% at 3 years. The limb salvage rate was 68.6% at 1 and at 3 years.", "target": "yes", "year": "2000", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "12607666", "question": "Is extended aortic replacement in acute type A dissection justifiable?", "context": "The aim of this study was to evaluate the effectiveness of our surgical strategy for acute aortic dissection based on the extent of the dissection and the site of the entry, with special emphasis on resection of all dissected aortic segments if technically possible.\n\nBetween January 1995 and March 2001, 43 consecutive patients underwent operations for acute aortic dissection. In all patients the distal repair was performed under circulatory arrest without the use of an aortic cross-clamp. Fifteen patients underwent aortic arch replacement with additional reconstruction of supra-aortic vessels in 3 patients. Complete replacement of all dissected tissue could be achieved in 21 patients (group 1). Because of the distal extent of the dissection beyond the aortic arch, replacement of all the dissected tissue was not possible in 22 patients (group 2).\n\nEarly mortality was 4.7% (2 patients), and the incidence of perioperative cerebrovascular events was 7.0% (3 patients). All of these events occurred in group 2 (p<0.025). During the follow-up period of 6 years or less, 5 patients died, all from causes not related to the aorta or the aortic valve. A persisting patent false lumen was observed in 14 of the 36 surviving patients (39%).", "target": "yes", "year": "2003", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "19836806", "question": "Should prostate specific antigen be adjusted for body mass index?", "context": "Obesity may be associated with lower prostate specific antigen through hemodilution. We examined the relationship between body mass index and prostate specific antigen by age in men without prostate cancer in a longitudinal aging study to determine whether prostate specific antigen must be adjusted for body mass index.\n\nThe study population included 994 men (4,937 observations) without prostate cancer in the Baltimore Longitudinal Study of Aging. Mixed effects models were used to examine the relationship between prostate specific antigen and body mass index in kg/m(2) by age. Separate models were explored in men with prostate cancer censored at diagnosis, for percent body fat measurements, for weight changes with time and adjusting for initial prostate size in 483 men (2,523 observations) with pelvic magnetic resonance imaging measurements.\n\nIn men without prostate cancer body mass index was not significantly associated with prostate specific antigen after adjusting for age (p = 0.06). A 10-point body mass index increase was associated with a prostate specific antigen difference of -0.03 ng/ml (95% CI -0.40-0.49). Results were similar when men with prostate cancer were included, when percent body fat was substituted for body mass index, and after adjusting for prostate volume. Longitudinal weight changes also had no significant association with prostate specific antigen.", "target": "no", "year": "2009", "labels": ["PURPOSE", "MATERIALS AND METHODS", "RESULTS"]}
{"id": "15708048", "question": "Does prior benign prostate biopsy predict outcome for patients treated with radical perineal prostatectomy?", "context": "To determine the effect of prior benign prostate biopsies on the surgical and clinical outcomes of patients treated with radical perineal prostatectomy for prostate cancer.\n\nA total of 1369 patients with clinically localized prostate cancer underwent radical prostatectomy by a single surgeon between 1991 and 2001. A subset of 203 patients (14.9%), who had undergone at least one prior benign prostate biopsy for a rising prostate-specific antigen and/or abnormal digital rectal examination, constituted our study population. A total of 1115 patients with no prior biopsy represented our control group. After prostatectomy, patients were evaluated at 6-month intervals for biochemical evidence of recurrence, defined as a prostate-specific antigen level of 0.5 ng/mL or greater.\n\nPatients with a prior benign biopsy had more favorable pathologic features with more organ-confined (74% versus 64%; P<0.001) and less margin-positive (9.8% versus 18%) disease. Only 24 patients (12%) in the study group (versus 20% in control group; P = 0.01) had eventual evidence of biochemical failure. Kaplan-Meier analyses suggested that patients with prior benign biopsies have improved biochemical disease-free survival, especially for those with more aggressive disease (Gleason sum 7 or greater; P<0.01). Overall, patients in the study group had lower probability (odds ratio 0.57, P<0.001) of biochemical failure compared with those in the control group.", "target": "yes", "year": "2005", "labels": ["OBJECTIVES", "METHODS", "RESULTS"]}
{"id": "9363244", "question": "Does occupational nuclear power plant radiation affect conception and pregnancy?", "context": "To determine the effect of occupational exposure in a nuclear power plant in Griefswald, Germany on male and female fecundity.\n\nThe frequency of men and women exposed to ionizing radiation through work in a nuclear power plant among 270 infertile couples was retrospectively compared to a control fertile population using a pair-matched analysis. The total cumulative equivalent radiation dose was determined. In addition, the spermiograms of the male partners in both groups were compared and correlated to the degree of exposure.\n\nNo differences were noted in the frequency of nuclear power plant exposure between sterile and fertile groups. There was a higher rate of anomalous spermiograms in nuclear power plant workers. However, abnormalities were temporary. No correlation was found between the cumulative equivalent radiation dose and abnormal spermiograms.", "target": "yes", "year": "1995", "labels": ["OBJECTIVES", "METHODS", "RESULTS"]}
{"id": "17971187", "question": "Cholesterol screening in school children: is family history reliable to choose the ones to screen?", "context": "The study was carried on 2096 school children (1043 male, 1053 female) in Ankara. Their mean age was 9.03 years. Demographic properties of the study group and their families were determined and the serum lipid levels of the subjects were obtained. The relation between these demographic properties and lipid levels were investigated.\n\nIn 135 of the subjects' serum cholesterol level was>or=200 mg/dL and in 83 subjects serum LDL-cholesterol level was>or=130 mg/dL. Despite 64.4% of the subjects reported a family history of hyperlipidemia, no relations between family history and serum lipid levels were found.", "target": "no", "year": "2007", "labels": ["METHODS", "RESULTS"]}
{"id": "26298839", "question": "Is Acupuncture Efficacious for Treating Phonotraumatic Vocal Pathologies?", "context": "To investigate the effectiveness of acupuncture in treating phonotraumatic vocal fold lesions.STUDY DESIGN/\n\nA total of 123 dysphonic individuals with benign vocal pathologies were recruited. They were given either genuine acupuncture (n = 40), sham acupuncture (n = 44), or no treatment (n = 39) for 6 weeks (two 30-minute sessions/wk). The genuine acupuncture group received needles puncturing nine voice-related acupoints for 30 minutes, two times a week for 6 weeks, whereas the sham acupuncture group received blunted needles stimulating the skin surface of the nine acupoints for the same frequency and duration. The no-treatment group did not receive any intervention but attended just the assessment sessions. One-hundred seventeen subjects completed the study (genuine acupuncture = 40; sham acupuncture = 43; and no treatment = 34), but only 84 of them had a complete set of vocal functions and quality of life measures (genuine acupuncture = 29; sham acupuncture = 33; and no-treatment = 22) and 42 of them with a complete set of endoscopic data (genuine acupuncture = 16; sham acupuncture = 15; and no treatment = 11).\n\nSignificant improvement in vocal function, as indicated by the maximum fundamental frequency produced, and also perceived quality of life, were found in both the genuine and sham acupuncture groups, but not in the no-treatment group. Structural (morphological) improvements were, however, only noticed in the genuine acupuncture group, which demonstrated a significant reduction in the size of the vocal fold lesions.", "target": "yes", "year": "2016", "labels": ["OBJECTIVES", "METHODS", "RESULTS"]}
{"id": "21592383", "question": "Canada's Compassionate Care Benefit: is it an adequate public health response to addressing the issue of caregiver burden in end-of-life care?", "context": "An increasingly significant public health issue in Canada, and elsewhere throughout the developed world, pertains to the provision of adequate palliative/end-of-life (P/EOL) care. Informal caregivers who take on the responsibility of providing P/EOL care often experience negative physical, mental, emotional, social and economic consequences. In this article, we specifically examine how Canada's Compassionate Care Benefit (CCB)--a contributory benefits social program aimed at informal P/EOL caregivers--operates as a public health response in sustaining informal caregivers providing P/EOL care, and whether or not it adequately addresses known aspects of caregiver burden that are addressed within the population health promotion (PHP) model.\n\nAs part of a national evaluation of Canada's Compassionate Care Benefit, 57 telephone interviews were conducted with Canadian informal P/EOL caregivers in 5 different provinces, pertaining to the strengths and weaknesses of the CCB and the general caregiving experience. Interview data was coded with Nvivo software and emerging themes were identified by the research team, with such findings published elsewhere. The purpose of the present analysis was identified after comparing the findings to the literature specific to caregiver burden and public health, after which data was analyzed using the PHP model as a guiding framework.\n\nInformal caregivers spoke to several of the determinants of health outlined in the PHP model that are implicated in their burden experience: gender, income and social status, working conditions, health and social services, social support network, and personal health practises and coping strategies. They recognized the need for improving the CCB to better address these determinants.", "target": "no", "year": "2011", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "19156007", "question": "Can clinicians use the PHQ-9 to assess depression in people with vision loss?", "context": "To investigate whether the Patient Health Questionnaire-9 (PHQ-9) possesses the essential psychometric characteristics to measure depressive symptoms in people with visual impairment.\n\nThe PHQ-9 scale was completed by 103 participants with low vision. These data were then assessed for fit to the Rasch model.\n\nThe participants' mean +/- standard deviation (SD) age was 74.7 +/- 12.2 years. Almost one half of them (n = 46; 44.7%) were considered to have severe vision impairment (presenting visual acuity<6/60 in the better eye). Disordered thresholds were evident initially. Collapsing the two middle categories produced ordered thresholds and fit to the Rasch model (chi = 10.1; degrees of freedom = 9; p = 0.34). The mean (SD) items and persons Fit Residual values were -0.31 (1.12) and -0.25 (0.78), respectively, where optimal fit of data to the Rasch model would have a mean = 0 and SD = 1. Unidimensionality was demonstrated confirming the construct validity of the PHQ-9 and there was no evidence of differential item functioning on a number of factors including visual disability. The person separation reliability value was 0.80 indicating that the PHQ-9 has satisfactory precision. There was a degree of mistargeting as expected in this largely non-clinically depressed sample.", "target": "yes", "year": "2009", "labels": ["PURPOSE", "METHODS", "RESULTS"]}
{"id": "11759976", "question": "Advanced epithelial ovarian carcinoma in Thai women: should we continue to offer second-look laparotomy?", "context": "To determine survival among patients with epithelial ovarian carcinoma (EOC) who underwent a second-look laparotomy (SLL) and those refusing the procedure. Also to analyze factor(s) influencing the survival of the patients.\n\nMedical records were reviewed of patients with advanced EOC who were clinically free of disease after primary surgery and platinum-based chemotherapy between January 1, 1992, and December 31, 1998. All of them were offered SLL. Measurement outcomes include patient survival and disease-free survival.\n\nThere were 50 patients with clinically complete remission after chemotherapy. Sixteen patients underwent SLL, and thirty-four patients refused the procedure (NSLL). Seven patients (43.8%) were reported to have positive SLL. After the median follow-up time of 35 months, 12 patients had died, and 5 patients were lost to follow-up. The median survival time for patients with SLL was about 60 months. Five-year survival rates of patients in the SLL, and NSLL groups were 37 per cent (95%CI = 7%-69%), and 88 per cent (95%CI = 65%-96%) respectively (P<0.001). The median time to relapse was about 25 months for patients with negative SLL. Five-year disease-free survival rates of patients in the negative SLL, and NSLL groups were 28 per cent (95%CI = 4%-59%), and 54 per cent (95%CI = 34%-70%) respectively (P=0.251). By Cox regression analysis, tumor grade was the only significant prognostic factor influencing patients' survival (HR = 6, 95%CI of HR = 1.2-34.2).", "target": "no", "year": "2001", "labels": ["OBJECTIVE", "METHOD AND MATERIAL", "RESULTS"]}
{"id": "9722752", "question": "Does bone anchor fixation improve the outcome of percutaneous bladder neck suspension in female stress urinary incontinence?", "context": "To evaluate the outcome of a new modification of percutaneous needle suspension, using a bone anchor system for fixing the suture at the public bone, and to compare the results with those published previously.\n\nFrom March 1996, 37 patients with stress urinary incontinence (>2 years) were treated using a bone anchor system. On each side the suture was attached to the pubocervical fascia and the vaginal wall via a broad 'Z'-stitch. A urodynamic investigation performed preoperatively in all patients confirmed stress incontinence and excluded detrusor instability. The outcome was assessed by either by a clinical follow-up investigation or using a standardized questionnaire, over a mean follow-up of 11 months (range 6-18).\n\nIn the 37 patients, the procedure was successful in 25 (68%), with 16 (43%) of the patients completely dry and nine (24%) significantly improved. Removal of the bone anchor and suture was necessary in two patients, because of unilateral bacterial infection in one and a bilateral soft tissue granuloma in the other. One bone anchor became dislocated in a third patient. In two cases where the treatment failed, new detrusor instability was documented urodynamically. Minor complications were prolonged wound pain in 10 (26%) and transient urinary retention or residual urine in 12 patients (32%).", "target": "yes", "year": "1998", "labels": ["OBJECTIVE", "PATIENTS AND METHODS", "RESULTS"]}
{"id": "10732884", "question": "Does coronary angiography before emergency aortic surgery affect in-hospital mortality?", "context": "To study the relationship between coronary angiography and in-hospital mortality in patients undergoing emergency surgery of the aorta without a history of coronary revascularization or coronary angiography before the onset of symptoms.\n\nIn the setting of acute ascending aortic dissection warranting emergency aortic repair, coronary angiography has been considered to be desirable, if not essential. The benefits of defining coronary anatomy have to be weighed against the risks of additional delay before surgical intervention.\n\nRetrospective analysis of patient charts and the Cardiovascular Information Registry (CVIR) at the Cleveland Clinic Foundation.\n\nWe studied 122 patients who underwent emergency surgery of the aorta between January 1982 and December 1997. Overall, in-hospital mortality was 18.0%, and there was no significant difference between those who had coronary angiography on the day of surgery compared with those who had not (No: 16%, n = 81 vs. Yes: 22%, n = 41, p = 0.46). Multivariate analysis revealed that a history of myocardial infarction (MI) was the only predictor of in-hospital mortality (relative risk: 4.98 95% confidence interval: 1.48-16.75, p = 0.009); however, coronary angiography had no impact on in-hospital mortality in patients with a history of MI. Furthermore, coronary angiography did not significantly affect the incidence of coronary artery bypass grafting (CABG) during aortic surgery (17% vs. 25%, Yes vs. No). Operative reports revealed that 74% of all CABG procedures were performed because of coronary dissection, and not coronary artery disease.", "target": "no", "year": "2000", "labels": ["OBJECTIVES", "BACKGROUND", "METHODS", "RESULTS"]}
{"id": "8017535", "question": "Substance use and HIV-related sexual behaviors among US high school students: are they related?", "context": "This study was undertaken to examine whether use of alcohol, cigarettes, marijuana, cocaine, and other illicit drugs is related to the likelihood of sexual behaviors that increase risk for human immunodeficiency virus (HIV) infection among youth.\n\nThe 1990 national Youth Risk Behavior Survey was used to collect self-reported information about a broad range of health risk behaviors from a representative sample of 11,631 high school students in the United States.\n\nStudents who reported no substance use were least likely to report having had sexual intercourse, having had four or more sex partners, and not having used a condom at last sexual intercourse. Adjusted for age, sex, and race/ethnicity, odds ratios for each of these sexual risk behaviors were greatest among students who had used marijuana, cocaine, or other illicit drugs. Students who had used only alcohol or cigarettes had smaller but still significant increases in the likelihood of having had sexual intercourse and of having had four or more sex partners.", "target": "yes", "year": "1994", "labels": ["OBJECTIVES", "METHODS", "RESULTS"]}
{"id": "10401824", "question": "Is laparoscopic reoperation for failed antireflux surgery feasible?", "context": "Laparoscopic techniques can be used to treat patients whose antireflux surgery has failed.\n\nCase series.\n\nTwo academic medical centers.\n\nForty-six consecutive patients, of whom 21 were male and 25 were female (mean age, 55.6 years; range, 15-80 years). Previous antireflux procedures were laparoscopic (21 patients), laparotomy (21 patients), thoracotomy (3 patients), and thoracoscopy (1 patient).\n\nThe cause of failure, operative and postoperative morbidity, and the level of follow-up satisfaction were determined for all patients.\n\nThe causes of failure were hiatal herniation (31 patients [67%]), fundoplication breakdown (20 patients [43%]), fundoplication slippage (9 patients [20%]), tight fundoplication (5 patients [11%]), misdiagnosed achalasia (2 patients [4%]), and displaced Angelchik prosthesis (2 patients [4%]). Twenty-two patients (48%) had more than 1 cause. Laparoscopic reoperative procedures were Nissen fundoplication (n = 22), Toupet fundoplication (n = 13), paraesophageal hernia repair (n = 4), Dor procedure (n = 2), Angelchik prosthesis removal (n = 2), Heller myotomy (n = 2), and the takedown of a wrap (n = 1). In addition, 18 patients required crural repair and 13 required paraesophageal hernia repair. The mean +/- SEM duration of surgery was 3.5+/-1.1 hours. Operative complications were fundus tear (n = 8), significant bleeding (n = 4), bougie perforation (n = 1), small bowel enterotomy (n = 1), and tension pneumothorax (n = 1). The conversion rate (from laparoscopic to an open procedure) was 20% overall (9 patients) but 0% in the last 10 patients. Mortality was 0%. The mean +/- SEM hospital stay was 2.3+/-0.9 days for operations completed laparoscopically. Follow-up was possible in 35 patients (76%) at 17.2+/-11.8 months. The well-being score (1 best; 10, worst) was 8.6+/-2.1 before and 2.9+/-2.4 after surgery (P<.001). Thirty-one (89%) of 35 patients were satisfied with their decision to have reoperation.", "target": "yes", "year": "1999", "labels": ["HYPOTHESIS", "DESIGN", "SETTING", "PATIENTS", "MAIN OUTCOME MEASURES", "RESULTS"]}
{"id": "24449622", "question": "Is there a relationship between serum paraoxonase level and epicardial fat tissue thickness?", "context": "This study aimed to show the relationship between serum paraoxonase 1 level and the epicardial fat tissue thickness.\n\nTwo hundred and seven patients without any atherosclerotic disease history were included in this cross-sectional observational study. Correlation analysis was performed to determine the correlation between epicardial fat tissue thickness, which was measured by echocardiography and serum paraoxonase 1 level. Also correlation analysis was performed to show correlation between patients' clinical and laboratory findings and the level of serum paraoxonase 1 (PON 1) and the epicardial fat tissue thickness. Pearson and Spearman test were used for correlation analysis.\n\nNo linear correlation between epicardial fat tissue thickness and serum PON 1 found (correlation coefficient: -0.127, p=0.069). When epicardial fat tissue thickness were grouped as 7 mm and over, and below, and 5 mm and over, and below, serum PON 1 level were significantly lower in ≥7 mm group (PON1 : 168.9 U/L) than<7 mm group (PON 1: 253.9 U/L) (p<0.001). Also hypertension prevalence was increased in ≥7 mm group (p=0.001). Serum triglyceride was found to be higher in ≥7 mm group (p=0.014), body mass index was found higher in ≥5 mm group (p=0.006).", "target": "no", "year": "2014", "labels": ["OBJECTIVE", "METHODS", "RESULTS"]}
{"id": "12095973", "question": "Chemoradiation instead of surgery to treat mid and low rectal tumors: is it safe?", "context": "The main treatment for rectal carcinoma is surgery. Preoperative chemoradiation (CRT) is advocated to reduce local recurrence and improve resection of mid and low tethered rectal tumors.\n\nFifty-two patients with mid or low rectal tumors underwent CRT (external beam radiation plus 5-fluorouracil plus folinic acid). Patients who had low rectal tumors with complete response (CR) were not submitted to surgical treatment. All other patients were submitted to surgery, independently of the response. Mean follow-up was 32.1 months.\n\nFive-year overall survival was 60.5%. Clinical evaluation after CRT showed CR in 10 cases (19.2%), all low tumors; incomplete response (>50%) in 21 (40.4%); and no response (<50%) in 19 (36.6%). Among the 10 cases with CR, 8 presented with local recurrence within 3.7 to 8.8 months. Two patients were not submitted to surgery and are still alive without cancer after 37 and 58 months. Thirty-nine patients had radical surgery. Seven had local recurrences after CRT plus surgery (17.9%). Overall survival was negatively affected by lymph node metastases (P =.017) and perineural invasion (P =.026).", "target": "no", "year": "2002", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "10973547", "question": "Are patients with Werlhof's disease at increased risk for bleeding complications when undergoing cardiac surgery?", "context": "It is generally assumed, that patients with Werlhof's disease (WD) are at increased risk for bleeding complications when undergoing cardiac surgery with extracorporeal circulation. Therefore we performed this case control study to estimate the real risk for bleeding complications of these patients.\n\nBetween 05/95 and 07/98, ten patients with WD (eight males, two females) underwent cardiac surgery employing extracorporeal circulation (WD-group). Five of these patients with platelet counts below 80/nl were treated by immunoglobulins preoperatively. Each patient with WD was matched to five patients without WD (no-WD-group) using diagnosis, age, gender, ejection fraction, number of distal anastomosis and body-mass-index as matching criteria.\n\nMean number of platelet counts were significant lower in the WD-group than in the no-WD-group despite a significant increase of platelet counts after immunoglobulin treatment (54/nl-->112/nl, P=0.018). On the day before, directly after and on the first day after surgery they were 141/nl vs. 215/nl (P=0.012), 75/nl vs. 147/nl (P=0.001) and 93/nl vs. 136/nl (P=0.009). Accordingly, patients of the WD-group received significantly more platelet concentrates than patients of the no-WD-group (mean number of platelet concentrates: 2.3 versus 0.7, P=0.007). Total drainage loss via the mediastinal chest tubes was almost identical (1197 ml in the no-WD-group and 1140 ml in the WD-group). One patient of each group suffered from a bleeding complication requiring reexploration. Three patients of the no-WD-group (6%) and one patient of the WD-group (10%) expired postoperatively unrelated to WD.", "target": "no", "year": "2000", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "14627582", "question": "Double reading of barium enemas: is it necessary?", "context": "The purpose of our study was to determine the effectiveness, clinical impact, and feasibility of double reading barium enemas.\n\nIndependent double readings of 1,003 consecutive barium enemas (822 double- and 181 single-contrast examinations) were prospectively performed. From this pool of 1,003 examinations, 994 were included in our study. Examinations showing at least one polyp or carcinoma 5 mm or larger were considered to have positive results. For combined readings, results were considered positive if either of the two interpreters reported finding a polyp or carcinoma. A McNemar test was used to compare the first reader's results with the combined results of the first and second readers. Results were retrospectively correlated with endoscopic or surgical results in 360 patients, and agreement between first and combined readings and endoscopic results was determined.\n\nAdding a second reader increased the number of positive results on examinations from 249 to 315 (p<0.0001) and resulted in potential alteration of clinical treatment in 98 patients (9.9%). Sensitivity of the first and combined readings for detection of all lesions was identical, 76.3% (95% CI, 65.4-87.1%). Specificity decreased from 91.0% (95% CI, 87.9-94.3%) for the first reading to 86.4% (95% CI, 82.2-90.0%) for the combined reading. The overall measurement of agreement decreased from a kappa value of 61.8 (95% CI, 51.2-72.4%) for the first reading to 52.9 (95% CI, 42.2-63.6%) for the combined reading. The second reading required an average of 3.3 min. Sensitivity for the detection of adenocarcinomas was 100%.", "target": "no", "year": "2003", "labels": ["OBJECTIVE", "MATERIALS AND METHODS", "RESULTS"]}
{"id": "11340218", "question": "Does pretreatment with statins improve clinical outcome after stroke?", "context": "In primary and secondary prevention trials, statins have been shown to reduce the risk of stroke. In addition to lipid lowering, statins have a number of antiatherothrombotic and neuroprotective properties. In a preliminary observational study, we explored whether clinical outcome is improved in patients who are on treatment with statins when stroke occurs.\n\nWe conducted a population-based case-referent study of 25- to 74-year-old stroke patients with, for each case of a patient who was on statin treatment at the onset of stroke (n=125), 2 referent patients who were not treated with statins but were matched for age, gender, year of onset, and stroke subtype (n=250).\n\nThe unadjusted odds ratio for early discharge to home (versus late discharge or death) was 1.41 (95% CI 0.91 to 2.17) when patients on statin treatment were compared with referent stroke patients not on statins. Prognostic factors were, in general, more unfavorable among patients on statins. When this was adjusted for in a logistic regression model, the use of statins was a moderately strong but statistically nonsignificant predictor of discharge to home (multiple-adjusted odds ratio 1.42, 95% CI 0.90 to 2.22).", "target": "no", "year": "2001", "labels": ["BACKGROUND AND PURPOSE", "METHODS", "RESULTS"]}
{"id": "20082356", "question": "Should direct mesocolon invasion be included in T4 for the staging of gastric cancer?", "context": "One of the sites most frequently invaded by gastric cancer is the mesocolon; however, the UICC does not mention this anatomical site as an adjacent structure involved in gastric cancer. The purpose of this study was to characterize and classify mesocolon invasion from gastric cancer.\n\nWe examined 806 patients who underwent surgery for advanced gastric carcinoma from 1992 to 2007 at the Department of Surgery, Gangnam Severance Hospital, Korea. Among these, patients who showed macroscopically direct invasion into the mesocolon were compared to other patients with advanced gastric cancer.\n\nThe curability, number and extent of nodal metastasis, and the survival of the mesocolon invasion group were significantly worse than these factors in the T3 group. However, the survival of the mesocolon invasion group after curative resection was much better than that of patients who had incurable factors.", "target": "maybe", "year": "2010", "labels": ["BACKGROUND AND OBJECTIVES", "METHODS", "RESULTS"]}
{"id": "16097998", "question": "Is coeliac disease screening in risk groups justified?", "context": "The benefits of serologic screening for coeliac disease in asymptomatic individuals are debatable.AIM: To investigate dietary compliance, quality of life and bone mineral density after long-term treatment in coeliac disease patients found by screening in risk groups.\n\nThe study comprised 53 consecutive screen-detected coeliac patients diagnosed 14 years (median) ago. Dietary compliance was assessed by interview, 4-day food record and serology. Quality of life was evaluated by the Psychological General Well-Being and SF-36 questionnaires, gastrointestinal symptoms by the Gastrointestinal Symptom Rating Scale and bone mineral density by dual-energy x-ray absorptiometry. Comparisons were made to 44 symptom-detected-treated coeliac patients, 110 non-coeliac subjects and the general population.\n\nA total of 96% of screen-detected and 93% of symptom-detected coeliac patients adhered to a strict or fairly strict gluten-free diet. In screen-detected patients, quality of life and gastrointestinal symptoms were similar to those in symptom-detected patients or non-coeliac controls and bone mineral density was similar to that in the general population.", "target": "yes", "year": "2005", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "25940336", "question": "Does Residency Selection Criteria Predict Performance in Orthopaedic Surgery Residency?", "context": "More than 1000 candidates applied for orthopaedic residency positions in 2014, and the competition is intense; approximately one-third of the candidates failed to secure a position in the match. However, the criteria used in the selection process often are subjective and studies have differed in terms of which criteria predict either objective measures or subjective ratings of resident performance by faculty.QUESTIONS/\n\nDo preresidency selection factors serve as predictors of success in residency? Specifically, we asked which preresidency selection factors are associated or correlated with (1) objective measures of resident knowledge and performance; and (2) subjective ratings by faculty.\n\nCharts of 60 orthopaedic residents from our institution were reviewed. Preresidency selection criteria examined included United States Medical Licensing Examination (USMLE) Step 1 and Step 2 scores, Medical College Admission Test (MCAT) scores, number of clinical clerkship honors, number of letters of recommendation, number of away rotations, Alpha Omega Alpha (AOA) honor medical society membership, fourth-year subinternship at our institution, and number of publications. Resident performance was assessed using objective measures including American Board of Orthopaedic Surgery (ABOS) Part I scores and Orthopaedics In-Training Exam (OITE) scores and subjective ratings by faculty including global evaluation scores and faculty rankings of residents. We tested associations between preresidency criteria and the subsequent objective and subjective metrics using linear correlation analysis and Mann-Whitney tests when appropriate.\n\nObjective measures of resident performance namely, ABOS Part I scores, had a moderate linear correlation with the USMLE Step 2 scores (r = 0.55, p<0.001) and number of clinical honors received in medical school (r = 0.45, p<0.001). OITE scores had a weak linear correlation with the number of clinical honors (r = 0.35, p = 0.009) and USMLE Step 2 scores (r = 0.29, p = 0.02). With regards to subjective outcomes, AOA membership was associated with higher scores on the global evaluation (p = 0.005). AOA membership also correlated with higher global evaluation scores (r = 0.60, p = 0.005) with the strongest correlation existing between AOA membership and the \"interpersonal and communication skills\" subsection of the global evaluations.", "target": "yes", "year": "2016", "labels": ["BACKGROUND", "PURPOSES", "METHODS", "RESULTS"]}
{"id": "14599616", "question": "Can a practicing surgeon detect early lymphedema reliably?", "context": "Lymphedema may be identified by simpler circumference changes as compared with changes in limb volume.\n\nNinety breast cancer patients were prospectively enrolled in an academic trial, and seven upper extremity circumferences were measured quarterly for 3 years. A 10% volume increase or greater than 1 cm increase in arm circumference identified lymphedema with verification by a lymphedema specialist. Sensitivity and specificity of several different criteria for detecting lymphedema were compared using the academic trial as the standard.\n\nThirty-nine cases of lymphedema were identified by the academic trial. Using a 10% increase in circumference at two sites as the criterion, half the lymphedema cases were detected (sensitivity 37%). When using a 10% increase in circumference at any site, 74.4% of cases were detected (sensitivity 49%). Detection by a 5% increase in circumference at any site was 91% sensitive.", "target": "maybe", "year": "2003", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "25779009", "question": "Bactericidal activity of 3 cutaneous/mucosal antiseptic solutions in the presence of interfering substances: Improvement of the NF EN 13727 European Standard?", "context": "There is no standard protocol for the evaluation of antiseptics used for skin and mucous membranes in the presence of interfering substances. Our objective was to suggest trial conditions adapted from the NF EN 13727 standard, for the evaluation of antiseptics used in gynecology and dermatology.\n\nThree antiseptic solutions were tested in vitro: a chlorhexidine-benzalkonium (CB) combination, a hexamidine-chlorhexidine-chlorocresol (HCC) combination, and povidone iodine (P). The adaptation of trial conditions to the standard involved choosing dilutions, solvent, and interfering substances. The activity of solutions was assessed on the recommended strains at concentrations of 97% (pure solution), 50%, and 10% (diluted solution), and 1%. A logarithmic reduction ≥ 5 was expected after 60seconds of contact, to meet requirements of bactericidal activity.\n\nHCC did not present any bactericidal activity except on P. aeruginosa at a concentration of 97%. P was not bactericidal on E. hirae at any concentration and on S. aureus at 97%. CB had the most homogeneous bactericidal activity with a reduction>5 log on the 4 bacterial strains at concentrations of 97%, 50% and 10%.", "target": "maybe", "year": "2015", "labels": ["OBJECTIVE", "METHODS", "RESULTS"]}
{"id": "23386371", "question": "CPAP therapy in patients with idiopathic pulmonary fibrosis and obstructive sleep apnea: does it offer a better quality of life and sleep?", "context": "The recent literature shows an increased incidence of obstructive sleep apnea (OSA) in patients with idiopathic pulmonary fibrosis (IPF). On the other hand, there are no published studies related to continuous positive airway pressure (CPAP) treatment in this patient group. Our aim was to assess the effect of CPAP on sleep and overall life quality parameters in IPF patients with OSA and to recognize and overcome possible difficulties in CPAP initiation and acceptance by these patients.\n\nTwelve patients (ten males and two females, age 67.1 ± 7.2 years) with newly diagnosed IPF and moderate to severe OSA, confirmed by overnight attended polysomnography, were included. Therapy with CPAP was initiated after a formal in-lab CPAP titration study. The patients completed the Epworth Sleepiness Scale (ESS), the Pittsburgh Sleep Quality Index (PSQI), the Functional Outcomes in Sleep Questionnaire (FOSQ), the Fatigue Severity Scale (FSS), the SF-36 quality of life questionnaire, and the Beck Depression Inventory (BDI) at CPAP initiation and after 1, 3, and 6 months of effective CPAP therapy.\n\nA statistically significant improvement was observed in the FOSQ at 1, 3, and 6 months after CPAP initiation (baseline 12.9 ± 2.9 vs. 14.7 ± 2.6 vs. 15.8 ± 2.1 vs. 16.9 ± 1.9, respectively, p = 0.02). Improvement, although not statistically significant, was noted in ESS score (9.2 ± 5.6 vs. 7.6 ± 4.9 vs. 7.5 ± 5.3 vs. 7.7 ± 5.2, p = 0.84), PSQI (10.7 ± 4.4 vs. 10.1 ± 4.3 vs. 9.4 ± 4.7 vs. 8.6 ± 5.2, p = 0.66), FSS (39.5 ± 10.2 vs. 34.8 ± 8.5 vs. 33.6 ± 10.7 vs. 33.4 ± 10.9, p = 0.44), SF-36 (63.2 ± 13.9 vs. 68.9 ± 13.5 vs. 72.1 ± 12.9 vs. 74.4 ± 11.3, p = 0.27), and BDI (12.9 ± 5.5 vs. 10.7 ± 4.3 vs. 9.4 ± 4.8 vs. 9.6 ± 4.5, p = 0.40). Two patients had difficulty complying with CPAP for a variety of reasons (nocturnal cough, claustrophobia, insomnia) and stopped CPAP use after the first month, despite intense follow-up by the CPAP clinic staff. Heated humidification was added for all patients in order to improve the common complaint of disabling nocturnal cough.", "target": "maybe", "year": "2013", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "22900881", "question": "Should pulp chamber pulpotomy be seen as a permanent treatment?", "context": "Seventeen patients, aged 7-54 years (mean of 37.2 year), were treated by pulpotomy and filling with ProRoot MTA(®) in premolar or molar teeth with vital pulps and without clinical evidence of irreversible pulpitis. The patients were then followed up for 12 to 24 months and the teeth then assessed by clinical and radiographic examination. Statistical analysis was performed with Kaplan-Meier survival probability statistics to estimate the survival of the treated teeth.\n\nAt 24 months, the survival rate without any complementary treatment was estimated to be 82%. Two of the 17 treated teeth required root canal treatment for pain control and one for prosthetic reasons.", "target": "no", "year": "2013", "labels": ["METHODOLOGY", "RESULTS"]}
{"id": "24019262", "question": "Does high blood pressure reduce the risk of chronic low back pain?", "context": "Epidemiological studies have suggested inverse relationships between blood pressure and prevalence of conditions such as migraine and headache. It is not yet clear whether similar relationships can be established for back pain in particular in prospective studies.\n\nAssociations between blood pressure and chronic low back pain were explored in the cross-sectional HUNT 2 survey of a Norwegian county in 1995-1997, including 39,872 individuals who never used antihypertensive medication. A prospective study, comprising 17,209 initially back pain-free individuals and 5740 individuals reporting low back pain, was established by re-examinations in the HUNT 3 survey in 2006-2008. Associations were assessed by logistic regression with respect to systolic, diastolic and pulse pressure, with adjustment for education, work status, physical activity, smoking, body mass and lipid levels.\n\nIn the cross-sectional study, all three blood pressure measures showed inverse relationships with prevalence of low back pain in both sexes. In the prospective study of disease-free women, baseline pulse pressure and systolic pressure were inversely associated with risk of low back pain [odds ratio (OR) 0.93 per 10 mm Hg increase in pulse pressure, 95% confidence interval (CI) 0.89-0.98, p = 0.007; OR 0.95 per 10 mm Hg increase in systolic pressure, 95% CI 0.92-0.99, p = 0.005]. Results among men were equivocal. No associations were indicated with the occurrence of pain in individuals with low back pain at baseline.", "target": "yes", "year": "2014", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "24666444", "question": "Is there any evidence of a \"July effect\" in patients undergoing major cancer surgery?", "context": "The \"July effect\" refers to the phenomenon of adverse impacts on patient care arising from the changeover in medical staff that takes place during this month at academic medical centres in North America. There has been some evidence supporting the presence of the July effect, including data from surgical specialties. Uniformity of care, regardless of time of year, is required for patients undergoing major cancer surgery. We therefore sought to perform a population-level assessment for the presence of a July effect in this field.\n\nWe used the Nationwide Inpatient Sample to abstract data on patients undergoing 1 of 8 major cancer surgeries at academic medical centres between Jan. 1, 1999, and Dec. 30, 2009. The primary outcomes examined were postoperative complications and in-hospital mortality. Univariate analyses and subsequently multivariate analyses, controlling for patient and hospital characteristics, were performed to identify whether the time of surgery was an independent predictor of outcome after major cancer surgery.\n\nOn univariate analysis, the overall postoperative complication rate, as well as genitourinary and hematologic complications specifically, was higher in July than the rest of the year. However, on multivariate analysis, only hematologic complications were significantly higher in July, with no difference in overall postoperative complication rate or in-hospital mortality for all 8 surgeries considered separately or together.", "target": "no", "year": "2014", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "20577124", "question": "Is leptin involved in phagocytic NADPH oxidase overactivity in obesity?", "context": "Hyperleptinemia and oxidative stress play a major role in the development of cardiovascular diseases in obesity. This study aimed to investigate whether there is a relationship between plasma levels of leptin and phagocytic nicotinamide adenine dinucleotide phosphate (NADPH) oxidase activity, and its potential relevance in the vascular remodeling in obese patients.\n\nThe study was performed in 164 obese and 94 normal-weight individuals (controls). NADPH oxidase activity was evaluated by luminescence in phagocytic cells. Levels of leptin were quantified by ELISA in plasma samples. Carotid intima-media thickness (cIMT) was measured by ultrasonography. In addition, we performed in-vitro experiments in human peripheral blood mononuclear cells and murine macrophages.\n\nPhagocytic NADPH oxidase activity and leptin levels were enhanced (P<0.05) in obese patients compared with controls. NADPH oxidase activity positively correlated with leptin in obese patients. This association remained significant in a multivariate analysis. cIMT was higher (P<0.05) in obese patients compared with controls. In addition, cIMT also correlated positively with leptin and NADPH oxidase activity in obese patients. In-vitro studies showed that leptin induced NADPH oxidase activation. Inhibition of the leptin-induced NADPH oxidase activity by wortmannin and bisindolyl maleimide suggested a direct involvement of the phosphatidylinositol 3-kinase and protein kinase C pathways, respectively. Finally, leptin-induced NADPH oxidase activation promoted macrophage proliferation.", "target": "yes", "year": "2010", "labels": ["OBJECTIVES", "METHODS", "RESULTS"]}
{"id": "18439500", "question": "Laparoscopic myomectomy: do size, number, and location of the myomas form limiting factors for laparoscopic myomectomy?", "context": "To assess whether it is possible for an experienced laparoscopic surgeon to perform efficient laparoscopic myomectomy regardless of the size, number, and location of the myomas.\n\nProspective observational study (Canadian Task Force classification II-1).\n\nTertiary endoscopy center.\n\nA total of 505 healthy nonpregnant women with symptomatic myomas underwent laparoscopic myomectomy at our center. No exclusion criteria were based on the size, number, or location of myomas.\n\nLaparoscopic myomectomy and modifications of the technique: enucleation of the myoma by morcellation while it is still attached to the uterus with and without earlier devascularization.\n\nIn all, 912 myomas were removed in these 505 patients laparoscopically. The mean number of myomas removed was 1.85 +/- 5.706 (95% CI 1.72-1.98). In all, 184 (36.4%) patients had multiple myomectomy. The mean size of the myomas removed was 5.86 +/- 3.300 cm in largest diameter (95% CI 5.56-6.16 cm). The mean weight of the myomas removed was 227.74 +/- 325.801 g (95% CI 198.03-257.45 g) and median was 100 g. The median operating time was 60 minutes (range 30-270 minutes). The median blood loss was 90 mL (range 40-2000 mL). Three comparisons were performed on the basis of size of the myomas (<10 cm and>or=10 cm in largest diameter), number of myomas removed (<or=4 and>or=5 myomas), and the technique (enucleation of the myomas by morcellation while the myoma is still attached to the uterus and the conventional technique). In all these comparisons, although the mean blood loss, duration of surgery, and hospital stay were greater in the groups in which larger myomas or more myomas were removed or the modified technique was performed as compared with their corresponding study group, the weight and size of removed myomas were also proportionately larger in these groups. Two patients were given the diagnosis of leiomyosarcoma in their histopathology and 1 patient developed a diaphragmatic parasitic myoma followed by a leiomyoma of the sigmoid colon. Six patients underwent laparoscopic hysterectomy 4 to 6 years after the surgery for recurrent myomas. One conversion to laparotomy occurred and 1 patient underwent open subtotal hysterectomy for dilutional coagulopathy.", "target": "no", "year": null, "labels": ["STUDY OBJECTIVE", "DESIGN", "SETTING", "PATIENTS", "INTERVENTIONS", "MEASUREMENTS AND MAIN RESULTS"]}
{"id": "26215326", "question": "Does the clinical presentation of a prior preterm birth predict risk in a subsequent pregnancy?", "context": "The objective of the study was to determine whether risk of recurrent preterm birth differs based on the clinical presentation of a prior spontaneous preterm birth (SPTB): advanced cervical dilatation (ACD), preterm premature rupture of membranes (PPROM), or preterm labor (PTL).\n\nThis retrospective cohort study included singleton pregnancies from 2009 to 2014 complicated by a history of prior SPTB. Women were categorized based on the clinical presentation of their prior preterm delivery as having ACD, PPROM, or PTL. Risks for sonographic short cervical length and recurrent SPTB were compared between women based on the clinical presentation of their prior preterm birth. Log-linear regression was used to control for confounders.\n\nOf 522 patients included in this study, 96 (18.4%) had prior ACD, 246 (47.1%) had prior PPROM, and 180 (34.5%) had prior PTL. Recurrent PTB occurred in 55.2% of patients with a history of ACD compared with 27.2% of those with PPROM and 32.2% with PTL (P = .001). The mean gestational age at delivery was significantly lower for those with a history of ACD (34.0 weeks) compared with women with prior PPROM (37.2 weeks) or PTL (37.0 weeks) (P = .001). The lowest mean cervical length prior to 24 weeks was significantly shorter in patients with a history of advanced cervical dilation when compared with the other clinical presentations.", "target": "yes", "year": "2015", "labels": ["OBJECTIVE", "STUDY DESIGN", "RESULTS"]}
{"id": "15918864", "question": "Learning needs of postpartum women: does socioeconomic status matter?", "context": "Little is known about how information needs change over time in the early postpartum period or about how these needs might differ given socioeconomic circumstances. This study's aim was to examine women's concerns at the time of hospital discharge and unmet learning needs as self-identified at 4 weeks after discharge.\n\nData were collected as part of a cross-sectional survey of postpartum health outcomes, service use, and costs of care in the first 4 weeks after postpartum hospital discharge. Recruitment of 250 women was conducted from each of 5 hospitals in Ontario, Canada (n = 1,250). Women who had given vaginal birth to a single live infant, and who were being discharged at the same time as their infant, assuming care of their infant, competent to give consent, and able to communicate in one of the study languages were eligible. Participants completed a self-report questionnaire in hospital; 890 (71.2%) took part in a structured telephone interview 4 weeks after hospital discharge.\n\nApproximately 17 percent of participants were of low socioeconomic status. Breastfeeding and signs of infant illness were the most frequently identified concerns by women, regardless of their socioeconomic status. Signs of infant illness and infant care/behavior were the main unmet learning needs. Although few differences in identified concerns were evident, women of low socioeconomic status were significantly more likely to report unmet learning needs related to 9 of 10 topics compared with women of higher socioeconomic status. For most topics, significantly more women of both groups identified learning needs 4 weeks after discharge compared with the number who identified corresponding concerns while in hospital.", "target": "yes", "year": "2005", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "21889895", "question": "Will CT ordering practices change if we educate residents about the potential effects of radiation exposure?", "context": "The aim of this study was to determine if educating residents about the potential effects of radiation exposure from computed tomographic (CT) imaging alters ordering patterns. This study also explored whether referring physicians are interested in radiation education and was an initial effort to address their CT ordering behavior.\n\nTwo to four months after a radiologist's lecture on the potential effects of radiation exposure related to CT scans, urology and orthopedic residents were surveyed regarding the number and types of CT scans they ordered, the use of alternative imaging modalities, and whether they used the lecture information to educate patients.\n\nTwenty-one resident lecture attendants completed the survey. The number of CT scans ordered after the lecture stayed constant for 90% (19 of 21) and decreased for 10% (two of 21). The types of CT scans ordered changed after the lecture for 14% (three of 21). Thirty-three percent (seven of 21) reported increases in alternative imaging after the lecture, including 24% (five of 21) reporting increases in magnetic resonance imaging and 19% (four of 21) reporting increases in ultrasound. Patients directed questions about radiation exposure to 57% (12 of 21); 38% (eight of 21) used the lecture information to educate patients. Referring physicians were interested in the topic, and afterward, other physician groups requested radiation education lectures.", "target": "no", "year": "2011", "labels": ["RATIONALE AND OBJECTIVES", "MATERIALS AND METHODS", "RESULTS"]}
{"id": "7860319", "question": "Measuring hospital mortality rates: are 30-day data enough?", "context": "We compare 30-day and 180-day postadmission hospital mortality rates for all Medicare patients and those in three categories of cardiac care: coronary artery bypass graft surgery, acute myocardial infarction, and congestive heart failure. DATA SOURCES/\n\nHealth Care Financing Administration (HCFA) hospital mortality data for FY 1989.\n\nUsing hospital level public use files of actual and predicted mortality at 30 and 180 days, we constructed residual mortality measures for each hospital. We ranked hospitals and used receiver operating characteristic (ROC) curves to compare 0-30, 31-180, and 0-180-day postadmission mortality.\n\nFor the admissions we studied, we found a broad range of hospital performance when we ranked hospitals using the 30-day data; some hospitals had much lower than predicted 30-day mortality rates, while others had much higher than predicted mortality rates. Data from the time period 31-180 days postadmission yield results that corroborate the 0-30 day postadmission data. Moreover, we found evidence that hospital performance on one condition is related to performance on the other conditions, but that the correlation is much weaker in the 31-180-day interval than in the 0-30-day period. Using ROC curves, we found that the 30-day data discriminated the top and bottom fifths of the 180-day data extremely well, especially for AMI outcomes.", "target": "yes", "year": "1995", "labels": ["OBJECTIVE", "COLLECTION", "STUDY DESIGN", "PRINCIPAL FINDINGS"]}
{"id": "16769333", "question": "Preoperative tracheobronchoscopy in newborns with esophageal atresia: does it matter?", "context": "Despite surgical refinements, perioperative use of tracheobronchoscopy (TBS) as part of surgical approach to esophageal atresia (EA) is still controversial. The purpose of this study was to evaluate the influence of preoperative TBS in newborns with EA in preventing complications and improving diagnosis and surgical treatment.\n\nIn the period ranging from 1997 to 2003, 62 patients with EA underwent preoperative TBS. The procedure was carried out with flexible bronchoscope maintaining spontaneous breathing. When a wide carinal fistula was found, this was mechanically occluded by Fogarty catheter and cannulated with rigid bronchoscopy. Type of EA, surgical procedure variations caused by TBS, and associated anomalies not easily detectable were recorded.\n\nBefore TBS, the Gross classification of the 62 patients was as follows: type A, 9 patients; type B, none; type C, 51 patients. At TBS, however, 3 of 9 type A patients had an unsuspected proximal fistula (type B). These 3 patients, plus the 2 with H-type fistula, were repaired through a cervical approach. In 4 patients, previously undetected malformations of the respiratory tree (2 aberrant right upper bronchus and 2 hypoplastic bronchi) were found at TBS. Carinal fistulas in 14 type C patients were occluded by Fogarty catheter to improve ventilation during repair. No complications were observed. Overall, TBS was clinically useful in 28 (45.2%) of 62 patients, including 15 (24.2%) of 62 infants in whom it was crucial in modifying the surgical approach.", "target": "yes", "year": "2006", "labels": ["PURPOSE", "METHODS", "RESULTS"]}
{"id": "24450673", "question": "Delayed imaging in routine CT examinations of the abdomen and pelvis: is it worth the additional cost of radiation and time?", "context": "The purpose of this study was to retrospectively assess the potential benefits of delayed phase imaging series in routine CT scans of the abdomen and pelvis.\n\nRoutine contrast-enhanced abdominopelvic CT scans of 1000 consecutively examined patients (912 men, 88 women; average age, 60 years; range, 22-94 years) were retrospectively evaluated, and the added benefits of the delayed phase series through the abdomen were recorded for each examination. Examinations performed for indications requiring multiphasic imaging were excluded. Images were reviewed by two fellowship-trained abdominal radiologists, who were blinded to official CT reports. All examinations were performed between July 2008 and February 2010 at a single institution. Radiation doses for both the portal venous and delayed phases, when available, were analyzed to assess the effect of the delayed phase on overall radiation exposure.\n\nForty-two patients (4.2%) had findings that were further characterized or were observed only in the delayed phase. Most were incidental findings that could have been confirmed at noninvasive follow-up imaging, such as sonography or unenhanced CT or MRI. The most common findings were liver hemangioma (n = 12), adrenal adenoma (n = 12), and parapelvic renal cysts (n = 6). The most important finding was detection of a renal mass in one patient (0.1%). The mass was seen only on the delayed phase images but was difficult to appreciate in the portal venous phase. In the other 958 patients (95.8%), delayed imaging was of no benefit. In addition, use of the delayed phase resulted in a mean 59.5% increase in effective radiation dose.", "target": "no", "year": "2014", "labels": ["OBJECTIVE", "MATERIALS AND METHODS", "RESULTS"]}
{"id": "23899611", "question": "Attenuation of ischemia/reperfusion-induced ovarian damage in rats: does edaravone offer protection?", "context": "Twenty-eight female Sprague Dawley rats were allocated randomly to 4 groups. The sham group (group 1) was only subjected to catheter insertion, not to pneumoperitoneum. Group 2 received a 1 mg/kg dose of 0.9% sodium chloride by the intraperitoneal route for 10 min before pneumoperitoneum. Groups 3 and 4 received 6 and 12 mg/kg edaravone, respectively, by the intraperitoneal route for 10 min before pneumoperitoneum. After 60 min of pneumoperitoneum, the gas was deflated. Immediately after the reperfusion period, both ovaries were excised for histological scoring, caspase-3 immunohistochemistry and biochemical evaluation including glutathione (GSH) and malondialdehyde (MDA) levels. Also, total antioxidant capacity (TAC) was measured in plasma samples to evaluate the antioxidant effect of edaravone.\n\nOvarian sections in the saline group revealed higher scores for follicular degeneration and edema (p<0.0001) when compared with the sham group. Administration of different doses of edaravone in rats significantly prevented degenerative changes in the ovary (p<0.0001). Caspase-3 expression was only detected in the ovarian surface epithelium in all groups, and there was a significant difference between the treatment groups and the saline group (p<0.0001). Treatment of rats with edaravone reduced caspase-3 expression in a dose-dependent manner. Moreover, biochemical measurements of oxidative stress markers (MDA, GSH and TAC) revealed that prophylactic edaravone treatment attenuated oxidative stress induced by I/R injury.", "target": "yes", "year": "2013", "labels": ["METHODS", "RESULTS"]}
{"id": "22617083", "question": "Does age moderate the effect of personality disorder on coping style in psychiatric inpatients?", "context": "To examine age-related differences in the relationship between personality and coping strategies in an Australian population of psychiatric inpatients.\n\nConsenting eligible adults (N=238) from 18-100 years of age consecutively admitted to inpatient psychiatry units were assessed using the SCID I and II, the Coping Orientations to Problems Experienced Scale (COPE), the Brief Psychiatric Rating Scale (BPRS), the Global Assessment of Functioning Scale (GAF), the Social and Occupational Functioning Assessment Scale (SOFAS), the 12 Item Short-Form Heath Survey (SF12), the Sarason Social Support Questionnaire, and the NEO Five Factor Inventory (NEO-FFI) (cognitively impaired, and non-English speaking patients were excluded).\n\nOlder adults reported less symptomatology than younger patients and younger patients described more personality dysfunction than older patients. As assessed by the COPE, older adults reported lower levels of dysfunctional coping strategies than younger adults. Personality traits, social supports, gender, and age predicted coping strategies, while Axis I diagnosis, education, personality disorder, and symptom severity were not significant predictors of coping strategies.", "target": "yes", "year": "2012", "labels": ["OBJECTIVE", "METHOD", "RESULTS"]}
{"id": "16216859", "question": "Does a well developed collateral circulation predispose to restenosis after percutaneous coronary intervention?", "context": "To evaluate whether a well developed collateral circulation predisposes to restenosis after percutaneous coronary intervention (PCI).\n\nProspective observational study.\n\n58 patients undergoing elective single vessel PCI in a tertiary referral interventional cardiac unit in the UK.\n\nCollateral flow index (CFI) was calculated as (Pw-Pv)/(Pa-Pv), where Pa, Pw, and Pv are aortic, coronary wedge, and right atrial pressures during maximum hyperaemia. Collateral supply was considered poor (CFI<0.25) or good (CFI>or = 0.25).\n\nIn-stent restenosis six months after PCI, classified as neointimal volume>or = 25% stent volume on intravascular ultrasound (IVUS), or minimum lumen area<or = 50% stent area on IVUS, or minimum lumen diameter<or = 50% reference vessel diameter on quantitative coronary angiography.\n\nPatients with good collaterals had more severe coronary stenoses at baseline (90 (11)% v 75 (16)%, p<0.001). Restenosis rates were similar in poor and good collateral groups (35% v 43%, p = 0.76 for diameter restenosis, 27% v 45%, p = 0.34 for area restenosis, and 23% v 24%, p = 0.84 for volumetric restenosis). CFI was not correlated with diameter, area, or volumetric restenosis (r2<0.1 for each). By multivariate analysis, stent diameter, stent length,>10% residual stenosis, and smoking history were predictive of restenosis.", "target": "no", "year": "2006", "labels": ["OBJECTIVE", "DESIGN", "PATIENTS AND SETTING", "METHODS", "MAIN OUTCOME MEASURES", "RESULTS"]}
{"id": "29112560", "question": "Is the Distance Worth It?", "context": "It is unclear whether traveling long distances to high-volume centers would compensate for travel burden among patients undergoing rectal cancer resection.\n\nThe purpose of this study was to determine whether operative volume outweighs the advantages of being treated locally by comparing the outcomes of patients with rectal cancer treated at local, low-volume centers versus far, high-volume centers.\n\nThis was a population-based study.\n\nThe National Cancer Database was queried for patients with rectal cancer.\n\nPatients with stage II or III rectal cancer who underwent surgical resection between 2006 and 2012 were included.\n\nThe outcomes of interest were margins, lymph node yield, receipt of neoadjuvant chemoradiation, adjuvant chemotherapy, readmission within 30 days, 30-day and 90-day mortality, and 5-year overall survival.\n\nA total of 18,605 patients met inclusion criteria; 2067 patients were in the long-distance/high-volume group and 1362 in the short-distance/low-volume group. The median travel distance was 62.6 miles for the long-distance/high-volume group and 2.3 miles for the short-distance/low-volume group. Patients who were younger, white, privately insured, and stage III were more likely to have traveled to a high-volume center. When controlled for patient factors, stage, and hospital factors, patients in the short-distance/low-volume group had lower odds of a lymph node yield ≥12 (OR = 0.51) and neoadjuvant chemoradiation (OR = 0.67) and higher 30-day (OR = 3.38) and 90-day mortality (OR = 2.07) compared with those in the long-distance/high-volume group. The short-distance/low-volume group had a 34% high risk of overall mortality at 5 years compared with the long-distance/high-volume group.\n\nWe lacked data regarding patient and physician decision making and surgeon-specific factors.", "target": "yes", "year": "2017", "labels": ["BACKGROUND", "OBJECTIVE", "DESIGN", "SETTINGS", "PATIENTS", "MAIN OUTCOME MEASURES", "RESULTS", "LIMITATIONS"]}
{"id": "24866606", "question": "Do emergency ultrasound fellowship programs impact emergency medicine residents' ultrasound education?", "context": "Recent years have seen a rapid proliferation of emergency ultrasound (EUS) programs in the United States. To date, there is no evidence supporting that EUS fellowships enhance residents' ultrasound (US) educational experiences. The purpose of this study was to determine the impact of EUS fellowships on emergency medicine (EM) residents' US education.\n\nWe conducted a cross-sectional study at 9 academic medical centers. A questionnaire on US education and bedside US use was pilot tested and given to EM residents. The primary outcomes included the number of US examinations performed, scope of bedside US applications, barriers to residents' US education, and US use in the emergency department. The secondary outcomes were factors that would impact residents' US education. The outcomes were compared between residency programs with and without EUS fellowships.\n\nA total of 244 EM residents participated in this study. Thirty percent (95% confidence interval, 24%-35%) reported they had performed more than 150 scans. Residents in programs with EUS fellowships reported performing more scans than those in programs without fellowships (P = .04). Significant differences were noted in most applications of bedside US between residency programs with and without fellowships (P<.05). There were also significant differences in the barriers to US education between residency programs with and without fellowships (P<.05).", "target": "yes", "year": "2014", "labels": ["OBJECTIVES", "METHODS", "RESULTS"]}
{"id": "20197761", "question": "Is irritable bowel syndrome a diagnosis of exclusion?", "context": "Guidelines emphasize that irritable bowel syndrome (IBS) is not a diagnosis of exclusion and encourage clinicians to make a positive diagnosis using the Rome criteria alone. Yet many clinicians are concerned about overlooking alternative diagnoses. We measured beliefs about whether IBS is a diagnosis of exclusion, and measured testing proclivity between IBS experts and community providers.\n\nWe developed a survey to measure decision-making in two standardized patients with Rome III-positive IBS, including IBS with diarrhea (D-IBS) and IBS with constipation (C-IBS). The survey elicited provider knowledge and beliefs about IBS, including testing proclivity and beliefs regarding IBS as a diagnosis of exclusion. We surveyed nurse practitioners, primary care physicians, community gastroenterologists, and IBS experts.\n\nExperts were less likely than nonexperts to endorse IBS as a diagnosis of exclusion (8 vs. 72%; P<0.0001). In the D-IBS vignette, experts were more likely to make a positive diagnosis of IBS (67 vs. 38%; P<0.001), to perform fewer tests (2.0 vs. 4.1; P<0.01), and to expend less money on testing (US$297 vs. $658; P<0.01). Providers who believed IBS is a diagnosis of exclusion ordered 1.6 more tests and consumed $364 more than others (P<0.0001). Experts only rated celiac sprue screening and complete blood count as appropriate in D-IBS; nonexperts rated most tests as appropriate. Parallel results were found in the C-IBS vignette.", "target": "maybe", "year": "2010", "labels": ["OBJECTIVES", "METHODS", "RESULTS"]}
{"id": "24340838", "question": "Do ventricular arrhythmias in athletes subside over time?", "context": "Sudden death in athletes can occur during sport activities and is presumably related to ventricular arrhythmias.\n\nTo investigate the long-term follow-up ofathletes with ventricular arrhythmias during an exercise test.\n\nFrom a database of 56,462 athletes we identified 192 athletes (35 years old who had ventricular arrhythmias during an exercise test. Ninety athletes had>or =3 ventricular premature beats (VPB) (group A) and 102 athletes had ventricular couplets or non-sustained ventricular tachycardia during an exercise test (group B). A control group of 92 athletesfrom without ventricular arrhythmias was randomly seleclted from the database (group C). Of the 192 athletes 39 returnied for a repeat exercise test after a mean follow-up period of 70 +/- 25 months and they constitute the study population.\n\nTwelve athletes from group A, 21 fromgroup B and 6 from group C returned for a repeat exercise test. The athletes reached a significantly lower peak heart rate during their follow-up exercise test (P = 0.001). More athletes were engaged in competitive sports during their initialexercise test than in the follow-up test (P = 0.021). Most of theathletes who had VPB and/orventricular couplets and/or NSVT during their initial exercise test had far fewer ventricular arrhythmias in the follow-up exercise test (P = 0.001).", "target": "yes", "year": "2013", "labels": ["BACKGROUND", "OBJECTIVES", "METHODS", "RESULTS"]}
{"id": "19299238", "question": "Aromatase inhibitor-related musculoskeletal symptoms: is preventing osteoporosis the key to eliminating these symptoms?", "context": "Aromatase inhibitors (AIs) are an effective treatment for postmenopausal women with hormone receptor-positive breast cancer. However, patients receiving AIs report a higher incidence of musculoskeletal symptoms and bone fractures; the mechanism and risk factors for this correlation are not well studied. The aim of this study was to correlate these musculoskeletal symptoms and bone fractures in patients receiving AIs with bone mineral density (BMD), previous tamoxifen use, and administration of calcium/bisphosphonate (Ca/Bis).\n\nWe reviewed charts of 856 patients with hormone receptor-positive nonmetastatic breast cancer seen at our institution between January 1999 and October 2007. A total of 316 patients met the inclusion criteria of treatment with one of the AIs for>or = 3 months and availability of a dualenergy X-ray absorptiometry (DEXA) during this treatment. Arthralgia, generalized bone pain and/or myalgia, bone fracture after beginning AIs, any tamoxifen treatment, and Ca/Bis therapy were recorded.\n\nOur study demonstrates a significant association between symptoms and DEXA-BMD results (P<.001). Similarly, the group receiving tamoxifen before AIs had fewer patients with arthralgia or generalized bone pain/myalgia or bone fracture (P<.001). Furthermore, the group receiving AIs plus Ca/Bis had more patients without musculoskeletal symptoms and had fewer fractures. Finally, the group receiving steroidal AIs compared with nonsteroidal AIs had more patients with arthralgia or generalized bone pain and/or myalgia, and bone fractures (P<.001).", "target": "yes", "year": "2009", "labels": ["BACKGROUND", "PATIENTS AND METHODS", "RESULTS"]}
{"id": "23224030", "question": "Do European people with type 1 diabetes consume a high atherogenic diet?", "context": "Individuals with type 1 diabetes have a high risk of developing cardiovascular diseases, and it has been reported that they consume a high atherogenic diet. We examined how nutrient intake and adherence to current European nutritional recommendations evolved in a large cohort of European individuals with type 1 diabetes over a period of 7 years.SUBJECTS/\n\nWe analysed data from the EURODIAB Prospective Complications Study, a European multicentre prospective cohort study. Standardized 3-day dietary records were employed in individuals with type 1 diabetes. One thousand one hundred and two patients (553 men, 549 women, baseline age 33 ± 10 years, duration 15 ± 9 years) had complete nutritional data available at baseline and after 7 years. We calculated mean differences in reported nutrients over time and adjusted these for age, gender, HbA1c and BMI with ANOVA models.\n\nCompared to baseline, there were minor changes in nutrients. Reported protein (-0.35% energy (en), fat (-1.07% en), saturated fat (-0.25% en) and cholesterol (-7.42 mg/1000 kcal) intakes were lower, whereas carbohydrate (+1.23% en) and fibre (+0.46 g/1000 kcal) intakes were higher at the 7-year follow-up. European recommendations for adequate nutrient intakes were followed in individuals with type 1 diabetes for protein (76% at baseline and 78% at follow-up), moderately for fat (34, 40%), carbohydrate (34, 41%) and cholesterol (39, 47%), but poorly for fibre (1.4, 2.4%) and saturated fat (11, 13%).", "target": "yes", "year": "2013", "labels": ["OBJECTIVES", "METHODS", "RESULTS"]}
{"id": "17462393", "question": "Does normothermic normokalemic simultaneous antegrade/retrograde perfusion improve myocardial oxygenation and energy metabolism for hypertrophied hearts?", "context": "Beating-heart valve surgery appears to be a promising technique for protection of hypertrophied hearts. Normothermic normokalemic simultaneous antegrade/retrograde perfusion (NNSP) may improve myocardial perfusion. However, its effects on myocardial oxygenation and energy metabolism remain unclear. The present study was to determine whether NNSP improved myocardial oxygenation and energy metabolism of hypertrophied hearts relative to normothermic normokalemic antegrade perfusion (NNAP).\n\nTwelve hypertrophied pig hearts underwent a protocol consisting of three 20-minute perfusion episodes (10 minutes NNAP and 10 minutes NNSP in a random order) with each conducted at a different blood flow in the left anterior descending coronary artery (LAD [100%, 50%, and 20% of its initial control]). Myocardial oxygenation was assessed using near-infrared spectroscopic imaging. Myocardial energy metabolism was monitored using localized phosphorus-31 magnetic resonance spectroscopy.\n\nWith 100% LAD flow, both NNAP and NNSP maintained myocardial oxygenation, adenosine triphosphate, phosphocreatine, and inorganic phosphate at normal levels. When LAD flow was reduced to 50% of its control level, NNSP resulted in a small but significant decrease in myocardial oxygenation and phosphocreatine, whereas those measurements did not change significantly during NNAP. With LAD flow further reduced to 20% of its control level, both NNAP and NNSP caused a substantial decrease in myocardial oxygenation, adenosine triphosphate, and phosphocreatine with an increase in inorganic phosphate. However, the changes were significantly greater during NNSP than during NNAP.", "target": "no", "year": "2007", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "18799291", "question": "Is the histidine triad nucleotide-binding protein 1 (HINT1) gene a candidate for schizophrenia?", "context": ": The histidine triad nucleotide-binding protein 1, HINT1, hydrolyzes adenosine 5'-monophosphoramidate substrates such as AMP-morpholidate. The human HINT1 gene is located on chromosome 5q31.2, a region implicated in linkage studies of schizophrenia. HINT1 had been shown to have different expression in postmortem brains between schizophrenia patients and unaffected controls. It was also found to be associated with the dysregulation of postsynaptic dopamine transmission, thus suggesting a potential role in several neuropsychiatric diseases.\n\n: In this work, we studied 8 SNPs around the HINT1 gene region using the Irish study of high density schizophrenia families (ISHDSF, 1350 subjects and 273 pedigrees) and the Irish case control study of schizophrenia (ICCSS, 655 affected subjects and 626 controls). The expression level of HINT1 was compared between the postmortem brain cDNAs from schizophrenic patients and unaffected controls provided by the Stanley Medical Research Institute.\n\n: We found nominally significant differences in allele frequencies in several SNPs for both ISHDSF and ICCSS samples in sex-stratified analyses. However, the sex effect differed between the two samples. In expression studies, no significant difference in expression was observed between patients and controls. However, significant interactions amongst sex, diagnosis and rs3864283 genotypes were observed.", "target": "no", "year": "2008", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "24625433", "question": "Are high flow nasal cannulae noisier than bubble CPAP for preterm infants?", "context": "Noise exposure in the neonatal intensive care unit is believed to be a risk factor for hearing loss in preterm neonates. Continuous positive airway pressure (CPAP) devices exceed recommended noise levels. High flow nasal cannulae (HFNC) are an increasingly popular alternative to CPAP for treating preterm infants, but there are no in vivo studies assessing noise production by HFNC.\n\nTo study whether HFNC are noisier than bubble CPAP (BCPAP) for preterm infants.\n\nAn observational study of preterm infants receiving HFNC or BCPAP. Noise levels within the external auditory meatus (EAM) were measured using a microphone probe tube connected to a calibrated digital dosimeter. Noise was measured across a range of frequencies and reported as decibels A-weighted (dBA).\n\nA total of 21 HFNC and 13 BCPAP noise measurements were performed in 21 infants. HFNC gas flows were 2-5 L/min, and BCPAP gas flows were 6-10 L/min with set pressures of 5-7 cm of water. There was no evidence of a difference in average noise levels measured at the EAM: mean difference (95% CI) of -1.6 (-4.0 to 0.9) dBA for HFNC compared to BCPAP. At low frequency (500 Hz), HFNC was mean (95% CI) 3.0 (0.3 to 5.7) dBA quieter than BCPAP. Noise increased with increasing BCPAP gas flow (p=0.007), but not with increasing set pressure. There was a trend to noise increasing with increasing HFNC gas flows.", "target": "no", "year": "2014", "labels": ["BACKGROUND", "OBJECTIVE", "METHODS", "RESULTS"]}
{"id": "14631523", "question": "Sub-classification of low-grade cerebellar astrocytoma: is it clinically meaningful?", "context": "The objectives were to identify prognostic factors for the survival of children with cerebellar astrocytoma, and to evaluate the reproducibility and prognostic value of histological sub-classification and grading.\n\nChildren aged 0-14 years treated in Denmark for a cerebellar astrocytoma in the period 1960-1984 were included and followed until January 2001 or until their death. The histological specimens from each patient were reviewed for revised grading and classification according to three different classification schemes: the WHO, the Kernohan and the Daumas-Duport grading systems.\n\nThe overall survival rate was 81% after a follow-up time of 15-40 years. The significant positive prognostic factors for survival were \"surgically gross-total removal\" of the tumour at surgery and location of the tumour in the cerebellum proper as opposed to location in the fourth ventricle. No difference in survival time was demonstrated when we compared pilocytic astrocytoma and fibrillary astrocytoma. Moreover, we found that the Kernohan and the WHO classification systems had no predictive value and that the Daumas-Duport system is unsuitable as a prognostic tool for low-grade posterior fossa astrocytomas.", "target": "no", "year": "2003", "labels": ["OBJECTIVES", "METHODS", "RESULTS"]}
{"id": "20297950", "question": "Proof of concept study: does fenofibrate have a role in sleep apnoea syndrome?", "context": "To investigate the effect of fenofibrate on sleep apnoea indices.\n\nProof-of-concept study comprising a placebo run-in period (1 week, 5 weeks if fibrate washout was required) and a 4-week randomized, double-blind treatment period. Thirty-four subjects (mean age 55 years, body mass index 34 kg/m 2 , fasting triglycerides 3.5 mmol/L) with diagnosed sleep apnoea syndrome not treated with continuous positive airways pressure were enrolled and randomized to once daily treatment with fenofibrate (145 mg NanoCrystal(R) tablet) or placebo. Overnight polysomnography, computerized attention/vigilance tests and blood sampling for measurement of lipids, insulin, fasting plasma glucose and fibrinogen were performed at the end of each study period.\n\nNCT00816829.\n\nAs this was an exploratory study, a range of sleep variables were evaluated. The apnoea/hypopnoea index (AHI) and percentage of time spent with arterial oxygen saturation (SpO(2))<90% were relevant as they have been evaluated in other clinical trials. Other variables included total apnoeas, hypopnoeas and oxygen desaturations, and non-cortical micro-awakenings related to respiratory events per hour.\n\nFenofibrate treatment significantly reduced the percentage of time with SpO(2)<90% (from 9.0% to 3.5% vs. 10.0% to 11.5% with placebo, p = 0.007), although there was no significant change in the AHI (reduction vs. control 14% (95%CI -47 to 40%, p = 0.533). Treatment reduced obstructive apnoeas (by 44%, from 18.5 at baseline to 15.0 at end of treatment vs. 29.0 to 30.5 on placebo, p = 0.048), and non-cortical micro-awakenings per hour (from 23.5 to 18.0 vs. 24.0 to 25.0 with placebo, p = 0.004). Other sleep variables were not significantly influenced by fenofibrate.\n\nExploratory study in patients with mild to moderate sleep apnoea, limited treatment duration; concomitant hypnotic treatment (35%); lack of correction for multiplicity of testing.", "target": "yes", "year": "2010", "labels": ["OBJECTIVE", "METHODS", "CLINICAL TRIAL REGISTRATION", "MAIN OUTCOME MEASURES", "RESULTS", "KEY LIMITATIONS"]}
{"id": "24487044", "question": "Pharmacologic regimens for knee osteoarthritis prevention: can they be cost-effective?", "context": "We sought to determine the target populations and drug efficacy, toxicity, cost, and initiation age thresholds under which a pharmacologic regimen for knee osteoarthritis (OA) prevention could be cost-effective.\n\nWe used the Osteoarthritis Policy (OAPol) Model, a validated state-transition simulation model of knee OA, to evaluate the cost-effectiveness of using disease-modifying OA drugs (DMOADs) as prophylaxis for the disease. We assessed four cohorts at varying risk for developing OA: (1) no risk factors, (2) obese, (3) history of knee injury, and (4) high-risk (obese with history of knee injury). The base case DMOAD was initiated at age 50 with 40% efficacy in the first year, 5% failure per subsequent year, 0.22% major toxicity, and annual cost of $1,000. Outcomes included costs, quality-adjusted life expectancy (QALE), and incremental cost-effectiveness ratios (ICERs). Key parameters were varied in sensitivity analyses.\n\nFor the high-risk cohort, base case prophylaxis increased quality-adjusted life-years (QALYs) by 0.04 and lifetime costs by $4,600, and produced an ICER of $118,000 per QALY gained. ICERs>$150,000/QALY were observed when comparing the base case DMOAD to the standard of care in the knee injury only cohort; for the obese only and no risk factors cohorts, the base case DMOAD was less cost-effective than the standard of care. Regimens priced at $3,000 per year and higher demonstrated ICERs above cost-effectiveness thresholds consistent with current US standards.", "target": "yes", "year": "2014", "labels": ["OBJECTIVE", "DESIGN", "RESULTS"]}
{"id": "28407529", "question": "Resection of colorectal liver metastases after second-line chemotherapy: is it worthwhile?", "context": "Patient outcome after resection of colorectal liver metastases (CLM) following second-line preoperative chemotherapy (PCT) performed for insufficient response or toxicity of the first-line, is little known and has here been compared to the outcome following first-line.\n\nFrom January 2005 to June 2013, 5624 and 791 consecutive patients of a prospective international cohort received 1 and 2 PCT lines before CLM resection (group 1 and 2, respectively). Survival and prognostic factors were analysed.\n\nAfter a mean follow-up of 30.1 months, there was no difference in survival from CLM diagnosis (median, 3-, and 5-year overall survival [OS]: 58.6 months, 76% and 49% in group 2 versus 58.9 months, 71% and 49% in group 1, respectively, P = 0.32). After hepatectomy, disease-free survival (DFS) was however shorter in group 2: 17.2 months, 27% and 15% versus 19.4 months, 32% and 23%, respectively (P = 0.001). Among the initially unresectable patients of group 1 and 2, no statistical difference in OS or DFS was observed. Independent predictors of worse OS in group 2 were positive primary lymph nodes, extrahepatic disease, tumour progression on second line, R2 resection and number of hepatectomies/year<50. Positive primary nodes, synchronous and bilateral metastases were predictors of shorter DFS. Initial unresectability did not impact OS or DFS in group 2.", "target": "no", "year": "2017", "labels": ["PURPOSE", "PATIENTS AND METHODS", "RESULTS"]}
{"id": "16816043", "question": "Do French lay people and health professionals find it acceptable to breach confidentiality to protect a patient's wife from a sexually transmitted disease?", "context": "To determine under what conditions lay people and health professionals find it acceptable for a physician to breach confidentiality to protect the wife of a patient with a sexually transmitted disease (STD).\n\nIn a study in France, breaching confidentiality in 48 scenarios were accepted by 144 lay people, 10 psychologists and 7 physicians. The scenarios were all possible combinations of five factors: severity of the disease (severe, lethal); time taken to discuss this with (little time, much time); intent to inform the spouse about the disease (none, one of these days, immediately); intent to adopt protective behaviours (no intent, intent); and decision to consult an expert in STDs (yes, no), 2 x 2 x 3 x 2 x 2. The importance and interactions of each factor were determined, at the group level, by performing analyses of variance and constructing graphs.\n\nThe concept of breaching confidentiality to protect a wife from her husband's STD was favoured much more by lay people and psychologists than by physicians (mean ratings 11.76, 9.28 and 2.90, respectively, on a scale of 0-22). The patient's stated intentions to protect his wife and to inform her of the disease had the greatest impact on acceptability. A cluster analysis showed groups of lay participants who found breaching confidentiality \"always acceptable\" (n = 14), \"depending on the many circumstances\" (n = 87), requiring \"consultation with an expert\" (n = 30) and \"never acceptable (n = 13)\".", "target": "maybe", "year": "2006", "labels": ["OBJECTIVE", "METHODS", "RESULTS"]}
{"id": "24698298", "question": "MR arthrography of the shoulder: do we need local anesthesia?", "context": "To assess pain intensity with and without subcutaneous local anesthesia prior to intraarticular administration of contrast medium for magnetic resonance arthrography (MRa) of the shoulder.\n\nThis single-center study was conducted after an IRB waiver of authorization, between January 2010 and December 2012. All patients provided written, informed consent for the procedure. Our prospectively populated institutional database was searched, based on our inclusion criteria. There were 249 outpatients (178 men and 71 women; mean age, 44.4 years ± 14.6; range, 15-79) who underwent MRa and were enrolled in this study. Patients were excluded if they had received surgery of the shoulder before MRa, had undergone repeated MRa of the same shoulder, and/or had undergone MRa of both shoulders on the same day. Patients were randomly assigned into one of three groups. Patients in group A (n=61) received skin infiltration with local anesthesia. Patients in control group B (n=92) and group C (n=96) did not receive local anesthesia. Pain levels were immediately assessed after the injection for MRa using a horizontal visual analog scale (VAS) that ranged from 0 to 10. To compare the pain scores of the three groups for male and female patients, a two-way analysis of variance was used. A p-value equal to or less than 0.05 was considered to indicate a significant result.\n\nPatients who received local anesthesia (group A) showed a mean pain level on the VAS of 2.6 ± 2.3. In patients who did not receive local anesthetics (groups B and C), a mean pain level on the VAS of 2.6 ± 2.2 and 2.7 ± 2.4 were detected, respectively. Between the three groups, no statistically significant difference in pain intensity was detected (p=.960). There were significant differences in subjective pain perception between men and women (p=.009). Moreover, the sex difference in all three groups was equal (p=.934).", "target": "no", "year": "2014", "labels": ["PURPOSE", "MATERIALS AND METHODS", "RESULTS"]}
{"id": "24013712", "question": "Preoperative platelet count in esophageal squamous cell carcinoma: is it a prognostic factor?", "context": "Platelet count is inversely related to prognosis in many cancers; however, its role in esophageal cancer is still controversial. The purpose of this study was to determine the prognostic value of preoperative platelet count in esophageal squamous cell carcinoma (ESCC).\n\nFrom January 2006 to December 2008, a retrospective analysis of 425 consecutive patients with ESCC was conducted. A receiver operating characteristic (ROC) curve for survival prediction was plotted to verify the optimum cutoff point for preoperative platelet count. Univariate and multivariate analyses were performed to evaluate the prognostic parameters.\n\nA ROC curve for survival prediction was plotted to verify the optimum cutoff point for platelet count, which was 205 (× 10(9)/L). Patients with platelet count ≤ 205 had a significantly better 5-year survival than patients with a platelet count>205 (60.7 vs. 31.6 %, P<0.001). The 5-year survival of patients either with platelet count ≤ 205 or>205 were similar (68.6 vs. 58.8 %, P = 0.085) when the nodes were negative. However, the 5-year survival of patients with platelet count ≤ 205 was better than that of patients with a platelet count>205 when the nodes were involved (32.0 vs. 12.7 %, P = 0.004). Multivariate analysis showed that platelet count (P = 0.013), T grade (P = 0.017), and N staging (P<0.001) were independent prognostic factors.", "target": "yes", "year": "2013", "labels": ["PURPOSE", "METHODS", "RESULTS"]}
{"id": "16151770", "question": "Memory-provoked rCBF-SPECT as a diagnostic tool in Alzheimer's disease?", "context": "Alzheimer's disease (AD) is a primary degenerative disease that progressively affects all brain functions, with devastating consequences for the patient, the patient's family and society. Rest regional cerebral blood flow (rCBF) could have a strategic role in differentiating between AD patients and normal controls, but its use for this purpose has a low discriminatory capacity. The purpose of this study was to evaluate whether the diagnostic sensitivity of rCBF single-photon emission computed tomography (SPECT) could be increased by using an episodic memory task provocation, i.e. memory-provoked rCBF-SPECT (MP-SPECT).\n\nEighteen persons (73.2+/-4.8 years) with mild AD and 18 healthy elderly (69.4+/-3.9 years) were included in the study. The subjects were injected with (99m)Tc-hexamethylpropylene amine oxime (HMPAO) during memory provocation with faces and names, followed by an rCBF-SPECT study. The rCBF (99m)Tc-HMPAO SPECT images were analysed using statistical parametric mapping (SPM2). Peaks with a false discovery rate corrected value of 0.05 were considered significant.\n\nOn MP-SPECT, the AD group showed a significant rCBF reduction in the left parietal cortex in comparison with healthy elderly. At rest, no significant group differences were seen.", "target": "yes", "year": "2006", "labels": ["PURPOSE", "METHODS", "RESULTS"]}
{"id": "17105833", "question": "Empiric treatment of uncomplicated urinary tract infection with fluoroquinolones in older women in Israel: another lost treatment option?", "context": "Current guidelines for the treatment of uncomplicated urinary tract infection (UTI) in women recommend empiric therapy with antibiotics for which local resistance rates do not exceed 10-20%. We hypothesized that resistance rates of Escherichia coli to fluoroquinolones may have surpassed this level in older women in the Israeli community setting.\n\nTo identify age groups of women in which fluoroquinolones may no longer be appropriate for empiric treatment of UTI.\n\nResistance rates for ofloxacin were calculated for all cases of uncomplicated UTI diagnosed during the first 5 months of 2005 in a managed care organization (MCO) in Israel, in community-dwelling women aged 41-75 years. The women were without risk factors for fluoroquinolone resistance. Uncomplicated UTI was diagnosed with a urine culture positive for E. coli. The data set was stratified for age, using 5 year intervals, and stratum-specific resistance rates (% and 95% CI) were calculated. These data were analyzed to identify age groups in which resistance rates have surpassed 10%.\n\nThe data from 1291 urine cultures were included. The crude resistance rate to ofloxacin was 8.7% (95% CI 7.4 to 10.2). Resistance was lowest among the youngest (aged 41-50 y) women (3.2%; 95% CI 1.11 to 5.18), approached 10% in women aged 51-55 years (7.1%; 95% CI 3.4 to 10.9), and reached 19.86% (95% CI 13.2 to 26.5) among the oldest women (aged 56-75 y).", "target": "maybe", "year": "2006", "labels": ["BACKGROUND", "OBJECTIVES", "METHODS", "RESULTS"]}
{"id": "22768311", "question": "Is human cytomegalovirus infection associated with hypertension?", "context": "Recent studies have implicated the human cytomegalovirus (HCMV) as a possible pathogen for causing hypertension. We aimed to study the association between HCMV infection and hypertension in the United States National Health and Nutrition Examination Survey (NHANES).\n\nWe analyzed data on 2979 men and 3324 women in the NHANES 1999-2002. We included participants aged 16-49 years who had valid data on HCMV infection and hypertension.\n\nOf the participants, 54.7% had serologic evidence of HCMV infection and 17.5% had hypertension. There were ethnic differences in the prevalence of HCMV infection (P<0.001) and hypertension (P<0.001). The prevalence of both increased with age (P<0.001). Before adjustment, HCMV seropositivity was significantly associated with hypertension in women (OR=1.63, 95% CI=1.25-2.13, P=0.001) but not in men. After adjustment for race/ethnicity, the association between HCMV seropositivity and hypertension in women remained significant (OR=1.55, 95% CI=1.20-2.02, P=0.002). Further adjustment for body mass index, diabetes status and hypercholesterolemia attenuated the association (OR=1.44, 95% CI=1.10-1.90, P=0.010). However, after adjusting for age, the association was no longer significant (OR=1.24, 95% CI=0.91-1.67, P=0.162).", "target": "no", "year": "2012", "labels": ["PURPOSE", "METHODS", "RESULTS"]}
{"id": "23025584", "question": "Does stress increase imitation of drinking behavior?", "context": "That alcohol consumption is strongly influenced by the drinking behavior of social company has been demonstrated in observational research. However, not everyone is equally vulnerable to other people's drinking, and it is important to unravel which factors underlie these individual differences. This study focuses on the role of psychosocial stress in attempting to explain individual differences in the propensity to imitate alcohol consumption.\n\nWith a 2 (confederate's drinking condition: alcohol vs. soda) × 2 (participant's stress condition: stress vs. no stress) experimental design, we tested whether the tendency to imitate other people's drinking was related to participants' induced stress levels. The young male adults (N = 106) were randomly assigned to each of the conditions. In each session, directly after the stress or no-stress period, confederates and participants entered a bar laboratory where we observed their drinking behavior. Prior to entering the session, confederates were instructed to drink alcohol or soda.\n\nParticipants in both stress and no-stress conditions consumed substantially more alcohol when confederates drank alcohol than when they drank soda. There was no difference in alcohol consumed between stress and no-stress conditions. No moderating effect of stress on the tendency to drink along with peers was found.", "target": "no", "year": "2013", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "20538207", "question": "Should temperature be monitorized during kidney allograft preservation?", "context": "It is generally considered that kidney grafts should be preserved at 4 degrees C during cold storage. However, actual temperature conditions are not known. We decided to study the temperature levels during preservation with the Biotainer storage can and Vitalpack transport pack.\n\nTemperature was monitored using the Thermobouton probe during preservation of pig kidneys, in the same conditions used with human grafts. The probe recorded the temperature level every 10 minutes during four days. We compared the results found with the new storage can with results obtained in the same conditions with the storage can formerly used by our team. We also studied the best position of the probe for temperature monitoring and the influence of the amount of ice within the transport pack on the temperature level. We then monitored the temperature during the conservation of actual human kidney grafts harvested at our institution from August 2007 to May 2008.\n\nThe temperature levels were the same regardless of the position of the probe within the transport pack. The lowest temperature was maintained during 15 hours, and the temperature level stayed below 5 degrees C for 57 hours with the new storage can. The former storage can maintained the lowest temperature level for 80 minutes, and temperature reached 5 degrees C after 10 hours 40 minutes. Temperature levels were similar when 2 or 4 kg of crushed ice were used. We observed similar results when monitoring the conservation of human grafts.", "target": "no", "year": "2010", "labels": ["GOAL", "MATERIAL", "RESULTS"]}
{"id": "22825590", "question": "Are behavioural risk factors to be blamed for the conversion from optimal blood pressure to hypertensive status in Black South Africans?", "context": "Longitudinal cohort studies in sub-Saharan Africa are urgently needed to understand cardiovascular disease development. We, therefore, explored health behaviours and conventional risk factors of African individuals with optimal blood pressure (BP) (≤ 120/80 mm Hg), and their 5-year prediction for the development of hypertension.\n\nThe Prospective Urban Rural Epidemiology study in the North West Province, South Africa, started in 2005 and included African volunteers (n = 1994; aged>30 years) from a sample of 6000 randomly selected households in rural and urban areas.\n\nAt baseline, 48% of the participants were hypertensive (≥ 140/90 mmHg). Those with optimal BP (n = 478) were followed at a success rate of 70% for 5 years (213 normotensive, 68 hypertensive, 57 deceased). Africans that became hypertensive smoked more than the normotensive individuals (68.2% vs 49.8%), and they also had a greater waist circumference [ratio of geometric means of 0.94 cm (95% CI: 0.86-0.99)] and greater amount of γ-glutamyltransferase [0.74 U/l (95% CI: 0.62-0.88)]at baseline. The 5-year change in BP was independently explained by baseline γ-glutamyltransferase [R(2) = 0.23, β = 0.13 U/l (95% CI: 0.01-0.19)]. Alcohol intake also predicted central systolic BP and carotid cross-sectional wall area (CSWA) at follow-up. Waist circumference was another predictor of BP changes [β = 0.18 cm (95% CI: 0.05-0.24)]and CSWA. HIV infection was inversely associated with increased BP.", "target": "yes", "year": "2012", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "11483547", "question": "Does the aggressive use of polyvalent antivenin for rattlesnake bites result in serious acute side effects?", "context": "To determine the incidence and severity of acute side effects from the use of polyvalent antivenin in victims of rattlesnake bites.\n\nWe retrospectively reviewed the records of all patients who presented with rattlesnake bites to a university teaching hospital during an 11-year period. From patient medical records, we extracted demographic data, clinical measurements, and outcomes during emergency department evaluation and subsequent hospitalization. Data regarding serum sickness were not collected.\n\nPrimary outcome variables were the occurrence of immediate hypersensitivity reaction to antivenin, the type of reaction, permanent disability at hospital discharge, and mortality.\n\nWe identified a total of 73 patients with rattlesnake bites during the study period. Bite envenomation was graded as nonenvenomated, 7 patients (10%); mild, 23 patients (32%); moderate, 32 patients (44%); and severe, 11 patients (15%). We identified 65 patients who received antivenin. Antivenin doses ranged from 1 to 30 vials per patient (mean, 12.0 +/- 6.0), for a total of 777 vials. In 43 patients (66%), 10 or more vials of antivenin were given. The mean number of vials of antivenin given to each snakebite grade were as follows: mild, 8.4 (+/-4.0); moderate, 11.8 (+/-5.7); and severe, 18.7 (+/-6.3). No deaths, amputations, or permanent disability from snakebite occurred in the patients receiving antivenin. Acute side effects of antivenin-occurring within the first 6 hours after administration-were seen in 12 patients (18%; 95% confidence interval, 10%-30%). Acute side effects consisted solely of urticaria in all but 1 patient (2%; 95% confidence interval, 0%-8%). This patient had a history of previous antivenin reaction and required a short course of intravenous epinephrine for blood pressure support. No other complications occurred.", "target": "no", "year": "2001", "labels": ["OBJECTIVE", "DESIGN", "OUTCOME MEASURES", "RESULTS"]}
{"id": "19643525", "question": "Can vaginal pH be measured from the wet mount slide?", "context": "To assess the accuracy of vaginal pH measurement on wet mount microscopy slides compared with direct measurements on fresh vaginal fluid. We also tested whether differences in accuracy were dependent on the sampling devices used or on the diagnosis of the vaginal infections.\n\nUsing a cotton swab, cytobrush or wooden spatula a vaginal fluid specimen was collected from 84 consecutive women attending a vulvo-vaginitis clinic. A pH strip (pH range 4-7, Merck) was brought in contact with the vaginal fluid on the sampling device and on the glass slide after adding one droplet of saline and performing microscopy by two different people unaware of the microscopy results of the clinical exam. Values were compared by Fisher exact and Student's t-tests.\n\npH measurement from microscopy slides after the addition of saline causes systematic increases of pH leading to false positive readings. This is true for all types of disturbance of the flora and infections studied, and was seen in the abnormal as well as in the normal or intermediate pH range.", "target": "no", "year": "2009", "labels": ["OBJECTIVES", "STUDY DESIGN", "RESULTS"]}
{"id": "26418796", "question": "Do Wound Cultures Give Information About the Microbiology of Blood Cultures in Severe Burn Patients?", "context": "Blood stream infection (BSI) and the subsequent development of sepsis are among the most common infection complications occurring in severe burn patients. This study was designed to evaluate the relationship between the burn wound flora and BSI pathogens.\n\nDocumentation of all bacterial and fungal wound and blood isolates from severe burn patients hospitalized in the burn unit and intensive care unit was obtained from medical records retrieved retrospectively from a computerized, hospital-wide database over a 13-year period. All data were recorded in relation to the Ryan score.\n\nOf 195 severe burn patients, 88 had at least 1 BSI episode. Transmission of the same pathogen from wound to blood was documented in 30% of the patients, with a rising BSI frequency as the Ryan score increased. There were a total of 263 bacteremic episodes in 88 study patients, 44% of blood isolates were documented previously in wound cultures, and transmission of the same pathogen from wound to blood was noted in 65% of bacteremic patients.", "target": "yes", "year": "2016", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "17224424", "question": "Effects of exercise training on heart rate and QT interval in healthy young individuals: are there gender differences?", "context": "The aim of the present study was to assess the effects of exercise training on heart rate, QT interval, and on the relation between ventricular repolarization and heart rate in men and women.\n\nA 24 h Holter recording was obtained in 80 healthy subjects (40 males) who differed for the degree of physical activity. Trained individuals showed a lower heart rate and a higher heart rate variability than sedentary subjects, independent of the gender difference in basal heart rate. Mean 24 h QTc was similar in trained and non-trained men, while a significant difference was observed between trained and non-trained women. Exercise training reduced the QT/RR slope in both genders. This effect on the QT/RR relation was more marked in women; in fact, the gender difference in the ventricular repolarization duration at low heart rate observed in sedentary subjects was no longer present among trained individuals.", "target": "yes", "year": "2007", "labels": ["AIMS", "METHODS AND RESULTS"]}
{"id": "18182265", "question": "Body diffusion-weighted MR imaging of uterine endometrial cancer: is it helpful in the detection of cancer in nonenhanced MR imaging?", "context": "In this study, the authors discussed the feasibility and value of diffusion-weighted (DW) MR imaging in the detection of uterine endometrial cancer in addition to conventional nonenhanced MR images.\n\nDW images of endometrial cancer in 23 patients were examined by using a 1.5-T MR scanner. This study investigated whether or not DW images offer additional incremental value to conventional nonenhanced MR imaging in comparison with histopathological results. Moreover, the apparent diffusion coefficient (ADC) values were measured in the regions of interest within the endometrial cancer and compared with those of normal endometrium and myometrium in 31 volunteers, leiomyoma in 14 patients and adenomyosis in 10 patients. The Wilcoxon rank sum test was used, with a p<0.05 considered statistically significant.\n\nIn 19 of 23 patients, endometrial cancers were detected only on T2-weighted images. In the remaining 4 patients, of whom two had coexisting leiomyoma, no cancer was detected on T2-weighted images. This corresponds to an 83% detection sensitivity for the carcinomas. When DW images and fused DW images/T2-weighted images were used in addition to the T2-weighted images, cancers were identified in 3 of the remaining 4 patients in addition to the 19 patients (overall detection sensitivity of 96%). The mean ADC value of endometrial cancer (n=22) was (0.97+/-0.19)x10(-3)mm(2)/s, which was significantly lower than those of the normal endometrium, myometrium, leiomyoma and adenomyosis (p<0.05).", "target": "yes", "year": "2009", "labels": ["OBJECTIVE", "METHODS AND MATERIALS", "RESULTS"]}
{"id": "15280782", "question": "Is unsafe sexual behaviour increasing among HIV-infected individuals?", "context": "The number of new diagnoses of HIV infection is rising in the northwestern hemisphere and it is becoming increasingly important to understand the mechanisms behind this trend.\n\nTo evaluate whether reported unsafe sexual behaviour among HIV- infected individuals is changing over time.\n\nParticipants in the Swiss HIV Cohort Study were asked about their sexual practices every 6 months for 3 years during regular follow-up of the cohort beginning on 1 April 2000.\n\n: Logistic regression models were fit using generalized estimating equations assuming a constant correlation between responses from the same individual.\n\nAt least one sexual behaviour questionnaire was obtained for 6545 HIV-infected individuals and the median number of questionnaires completed per individual was five. There was no evidence of an increase in reported unsafe sex over time in this population [odds ratio (OR), 1.0; 95% confidence interval (CI), 0.96-1.05]. Females (OR, 1.38; 95% CI, 1.19-1.60), 15-30 year olds (OR, 1.26; 95% CI, 1.09-1.47), those with HIV-positive partners (OR, 12.58; 95% CI, 10.84-14.07) and those with occasional partners (OR, 3.25; 95% CI, 2.87-3.67) were more likely to report unsafe sex. There was no evidence of a response bias over time, but individuals were less willing to leave questions about their sexual behaviour unanswered or ambiguous (OR, 0.93; 95% CI, 0.90-0.97).", "target": "no", "year": "2004", "labels": ["BACKGROUND", "OBJECTIVE", "DESIGN", "METHODS", "RESULTS"]}
{"id": "20684175", "question": "Vitamin D supplementation and regulatory T cells in apparently healthy subjects: vitamin D treatment for autoimmune diseases?", "context": "Epidemiological data show significant associations of vitamin D deficiency and autoimmune diseases. Vitamin D may prevent autoimmunity by stimulating naturally occurring regulatory T cells.\n\nTo elucidate whether vitamin D supplementation increases Tregs frequency (%Tregs) within circulating CD4+ T cells.\n\nWe performed an uncontrolled vitamin D supplementation trial among 50 apparently healthy subjects including supplementation of 140,000 IU at baseline and after 4 weeks (visit 1). The final follow-up visit was performed 8 weeks after the baseline examination (visit 2). Blood was drawn at each study visit to determine 25-hydroxyvitamin D levels and %Tregs. Tregs were characterized as CD4+CD25++ T cells with expression of the transcription factor forkhead box P3 and low or absent expression of CD127.\n\nForty-six study participants (65% females, mean age +/- SD 31 +/- 8 years) completed the trial. 25(OH)D levels increased from 23.9 +/- 12.9 ng/ml at baseline to 45.9 +/- 14.0 ng/ml at visit 1 and 58.0 +/- 15.1 ng/ml at visit 2. %Tregs at baseline were 4.8 +/- 1.4. Compared to baseline levels we noticed a significant increase of %Tregs at study visit 1 (5.9 +/- 1.7, P<0.001) and 2 (5.6 +/- 1.6, P<0.001).", "target": "yes", "year": "2010", "labels": ["BACKGROUND", "OBJECTIVES", "METHODS", "RESULTS"]}
{"id": "26399179", "question": "Eyelid-parotid metastasis: do we screen for coexisting masses?", "context": "To report three cases illustrating that it is not unusual for a primary eyelid tumour to metastasise to the parotid gland and vice versa.\n\nTwo patients with malignant parotid tumours underwent radical parotidectomy and presented subsequently with eyelid lesions. Biopsy showed that both eyelid lesions were histologically similar to the primary parotid tumour. A third patient was noted to have ipsilateral upper eyelid and parotid gland tumours. Histology and immunocytochemistry were used to differentiate the primary tumour and the metastasis.", "target": "yes", "year": "2015", "labels": ["OBJECTIVE", "CASE REPORTS"]}
{"id": "20073599", "question": "Do liquid-based preparations of urinary cytology perform differently than classically prepared cases?", "context": "The cytomorphology of liquid-based preparations in urine cytology is different than classic slide preparations.\n\nTo compare the performance of liquid-based preparation specimens to classically prepared urine specimens with a malignant diagnosis in the College of American Pathologists Interlaboratory Comparison Program in Nongynecologic Cytology.\n\nParticipant responses between 2000 and 2007 for urine specimens with a reference diagnosis of high-grade urothelial carcinoma/carcinoma in situ/dysplasia (HGUCA), squamous cell carcinoma, or adenocarcinoma were evaluated. ThinPrep and SurePath challenges were compared with classic preparations (smears, cytospins) for discordant responses.\n\nThere were 18 288 pathologist, 11 957 cytotechnologist, and 8086 \"laboratory\" responses available. Classic preparations comprised 90% (n = 34 551) of urine challenges; 9% (n = 3295) were ThinPrep and 1% (n = 485) were SurePath. Concordance to the general category of \"positive-malignant\" was seen in 92% of classic preparations, 96.5% of ThinPrep, and 94.6% of SurePath challenges (P<.001). These results were statistically different for the exact reference interpretation of HGUCA (P<.001) but not for adenocarcinoma (P = .22). Cytotechnologists demonstrate statistically better performance for the general category of \"positive-malignant\" compared with pathologists for all urinary slide types and for the exact reference interpretation of HGUCA (94% versus 91.1%; P<.001) but not adenocarcinoma (96.3% versus 95.8%; P = .77) or squamous cell carcinoma (93.6% versus 87.7%; P = .07).", "target": "yes", "year": "2010", "labels": ["CONTEXT", "OBJECTIVES", "DESIGN", "RESULTS"]}
{"id": "15530261", "question": "Does screening or surveillance for primary hepatocellular carcinoma with ultrasonography improve the prognosis of patients?", "context": "The purpose of this paper is to evaluate the efficacy of ultrasonographic screening for primary hepatocellular carcinoma.\n\nA total of 680 eligible cases were classified into three groups (surveillance, opportunistic, and symptomatic groups) according to their initial exposure. We used survival time, tumor morphology, and T staging as prognostic outcomes. The outcomes of screened/unscreened and sur veillance/nonsur veillance were compared with the use of the logistic regression model.\n\nThe adjusted odds ratios for the screened group versus the unscreened group, with 1-, 2-, and 3-year survival time being used as outcomes, were 0.33 (95% confidence interval [CI], 0.21-0.52), 0.33 (95% CI, 0.21-0.53), and 0.37 (95% CI, 0.23-0.61), respectively. The adjusted odds ratios for surveillance versus nonsurveillance were 0.58 (95% CI, 0.35-0.97), 0.45 (95% CI, 0.27-0.74), and 0.44 (95% CI, 0.26-0.73). The odds ratios were even smaller when tumor morphology or T stage was taken as the main outcome. All these results were statistically significant. There were significant gradient relationships between prognostic outcomes and extent of screening history.", "target": "yes", "year": null, "labels": ["PURPOSE", "PATIENTS AND METHODS", "RESULTS"]}
{"id": "14697414", "question": "Is there a favorable subset of patients with prostate cancer who develop oligometastases?", "context": "To analyze, retrospectively, the patterns and behavior of metastatic lesions in prostate cancer patients treated with external beam radiotherapy and to investigate whether patients with<or =5 lesions had an improved outcome relative to patients with>5 lesions.\n\nThe treatment and outcome of 369 eligible patients with Stage T1-T3aN0-NXM0 prostate cancer were analyzed during a minimal 10-year follow-up period. All patients were treated with curative intent to a mean dose of 65 Gy. The full history of any metastatic disease was documented for each subject, including the initial site of involvement, any progression over time, and patient survival.\n\nThe overall survival rate for the 369 patients was 75% at 5 years and 45% at 10 years. The overall survival rate of patients who never developed metastases was 90% and 81% at 5 and 10 years, respectively. However, among the 74 patients (20%) who developed metastases, the survival rate at both 5 and 10 years was significantly reduced (p<0.0001). The overall survival rate for patients who developed bone metastases was 58% and 27% at 5 and 10 years, respectively, and patients with bone metastases to the pelvis fared worse compared with those with vertebral metastases. With regard to the metastatic number, patients with<or =5 metastatic lesions had superior survival rates relative to those with>5 lesions (73% and 36% at 5 and 10 years vs. 45% and 18% at 5 and 10 years, respectively; p = 0.02). In addition, both the metastasis-free survival rate and the interval measured from the date of the initial diagnosis of prostate cancer to the development of bone metastasis were statistically superior for patients with<or =5 lesions compared with patients with>5 lesions (p = 0.01 and 0.02, respectively). However, the survival rate and the interval from the date of diagnosis of bone metastasis to the time of death for patients in both groups were not significantly different, statistically (p = 0.17 and 0.27, respectively).", "target": "yes", "year": "2004", "labels": ["OBJECTIVE", "METHODS AND MATERIALS", "RESULTS"]}
{"id": "9582182", "question": "Does the SCL 90-R obsessive-compulsive dimension identify cognitive impairments?", "context": "To investigate the relevance of the Symptom Checklist 90-R Obsessive-Compulsive subscale to cognition in individuals with brain tumor.\n\nA prospective study of patients assessed with a neuropsychological test battery.\n\nA university medical center.\n\nNineteen adults with biopsy-confirmed diagnoses of malignant brain tumors were assessed prior to aggressive chemotherapy.\n\nIncluded in the assessment were the Mattis Dementia Rating Scale, California Verbal Learning Test, Trail Making Test B, Symptom Checklist 90-R, Mood Assessment Scale, Beck Anxiety Inventory, and Chronic Illness Problem Inventory.\n\nThe SCL 90-R Obsessive-Compulsive subscale was not related to objective measures of attention, verbal memory, or age. It was related significantly to symptoms of depression (r = .81, P<.005), anxiety (r = .66, P<.005), and subjective complaints of memory problems (r = .75, P<.005). Multivariate analyses indicated that reported symptoms of depression contributed 66% of the variance in predicting SCL 90-R Obsessive-Compulsive Scores, whereas symptoms of anxiety contributed an additional 6% (P<.0001).", "target": "yes", "year": "1998", "labels": ["OBJECTIVE", "DESIGN", "SETTING", "PATIENTS", "MAIN OUTCOME MEASURES", "RESULTS"]}
{"id": "25636371", "question": "Is it possible to stop treatment with nucleos(t)ide analogs in patients with e-antigen negative chronic hepatitis B?", "context": "Treatment of HBeAg-negative chronic hepatitis B (CHB) with nucleos(t)ide analogues (NA) is usually indefinite, since the loss of HBsAg, as a criterion for its discontinuation, is a rare event. Recent evidence suggests that discontinuing NA therapy may be feasible in selected patients.\n\nTo analyze the rate of virological relapse in patients with HBeAg-negative CHB who discontinued treatment with NAs.\n\nWe performed a single-center observational study that included 140 patients with HBsAg-negative CHB. Twenty-two patients, who received only NAs, discontinued treatment for different reasons and were subsequently monitored. All had normal ALT and AST, undetectable DNA and absence of cirrhosis or significant comorbidities before stopping treatment.\n\nTwelve patients showed virologic relapse (54.54%). The mean interval between discontinuation and relapse was 6.38 months (± 1.9) (75% relapsed during the first 12 months after discontinuation). Five received adefovir, 1 lamivudine and adefovir, 1 tenofovir and 5 lamivudine alone. The mean treatment duration in this group was 38.5 months (± 4.5). The sustained response group had a higher mean age and longer treatment duration than patients with virologic relapse but these differences were not statistically significant.", "target": "maybe", "year": "2015", "labels": ["BACKGROUND", "OBJECTIVES", "METHODS", "RESULTS"]}
{"id": "23568387", "question": "Is bicompartmental knee arthroplasty more favourable to knee muscle strength and physical performance compared to total knee arthroplasty?", "context": "Bicompartmental knee arthroplasty features bone and ligament sparing as unicompartmental knee arthroplasty and is presumably better in the recovery of muscle strength and function compared to total knee arthroplasty (TKA) though not previously reported in the literature. The aim of the study was to compare isokinetic knee muscle strength and physical performance in patients who underwent either bicompartmental knee arthroplasty or TKA.\n\nEach of 24 patients (31 knees) was prospectively examined preoperatively, at 6 and 12 months after each surgery. Isokinetic knee extensor and flexor strength as well as position sense were measured using the Biodex system. Timed up and go test, stair climbing test, and the 6-min walk test were used to assess physical performance. The results of each group were also compared with those from the corresponding healthy control, respectively.\n\nDemography showed significant difference in the mean age between bicompartment (54.8 ± 5.6 years) and TKA groups (65.7 ± 6.7 years). Comparing between the two groups, knee extensor and flexor torque, hamstring/Quadriceps ratio, position sense, and physical performance were not significantly different preoperatively, at 6 and 12 months after surgery. In intra-group analysis, muscle strength and position sense at each time point were not different in both groups. In physical performance, both groups resulted in improvement in the 6-min walk test, and only TKA group showed enhancement in stair climbing test.", "target": "no", "year": "2013", "labels": ["PURPOSE", "METHODS", "RESULTS"]}
{"id": "17914515", "question": "Suturing of the nasal septum after septoplasty, is it an effective alternative to nasal packing?", "context": "To discuss and compare the results of suturing the nasal septum after septoplasty with the results of nasal packing.\n\nA prospective study, which was performed at Prince Hashem Military Hospital in Zarqa, Jordan and Prince Rashed Military Hospital in Irbid, Jordan between September 2005 and August 2006 included 169 consecutive patients that underwent septoplasty. The patients were randomly divided into 2 groups. After completion of surgery, the nasal septum was sutured in the first group while nasal packing was performed in the second group.\n\nThirteen patients (15.3%) in the first group and 11 patients (13%) in the second group had minor oozing in the first 24 hours, 4 patients (4.8%) had bleeding after removal of the pack in the second group. Four patients (4.8%) developed septal hematoma in the second group. Two patients (2.4%) had septal perforation in the second group. One patient (1.1%) in the first group, and 5 patients (5.9%) in the second group had postoperative adhesions. Five patients (5.9%) were found to have remnant deviated nasal septum in each group. The operating time was 4 minutes longer in the first group.", "target": "yes", "year": "2007", "labels": ["OBJECTIVE", "METHODS", "RESULTS"]}
{"id": "25735444", "question": "Rectal cancer threatening or affecting the prostatic plane: is partial prostatectomy oncologically adequate?", "context": "A multicentre, retrospective study was conducted of patients with rectal cancer threatening or affecting the prostatic plane, but not the bladder, judged by magnetic resonance imaging (MRI). The use of preoperative chemoradiotherapy and the type of urologic resection were correlated with the status of the pathological circumferential resection margin (CRM) and local recurrence.\n\nA consecutive series of 126 men with rectal cancer threatening (44) or affecting (82) the prostatic plane on preoperative staging and operated with local curative intent between 1998 and 2010 was analysed. In patients who did not have chemoradiotherapy but had a preoperative threatened anterior margin the CRM-positive rate was 25.0%. In patients who did not have preoperative chemoradiotherapy but did have an affected margin, the CRM-positive rate was 41.7%. When preoperative radiotherapy was given, the respective CRM infiltration rates were 7.1 and 20.7%. In patients having preoperative chemoradiotherapy followed by prostatic resection the rate of CRM positivity was 2.4%. Partial prostatectomy after preoperative chemoradiotherapy resulted in a free anterior CRM in all cases, but intra-operative urethral damage occurred in 36.4% of patients who underwent partial prostatectomy, resulting in a postoperative urinary fistula in 18.2% of patients.", "target": "maybe", "year": "2015", "labels": ["METHOD", "RESULTS"]}
{"id": "19054501", "question": "Is motion perception deficit in schizophrenia a consequence of eye-tracking abnormality?", "context": "Studies have shown that schizophrenia patients have motion perception deficit, which was thought to cause eye-tracking abnormality in schizophrenia. However, eye movement closely interacts with motion perception. The known eye-tracking difficulties in schizophrenia patients may interact with their motion perception.\n\nTwo speed discrimination experiments were conducted in a within-subject design. In experiment 1, the stimulus duration was 150 msec to minimize the chance of eye-tracking occurrence. In experiment 2, the duration was increased to 300 msec, increasing the possibility of eye movement intrusion. Regular eye-tracking performance was evaluated in a third experiment.\n\nAt 150 msec, speed discrimination thresholds did not differ between schizophrenia patients (n = 38) and control subjects (n = 33). At 300 msec, patients had significantly higher thresholds than control subjects (p = .03). Furthermore, frequencies of eye tracking during the 300 msec stimulus were significantly correlated with speed discrimination in control subjects (p = .01) but not in patients, suggesting that eye-tracking initiation may benefit control subjects but not patients. The frequency of eye tracking during speed discrimination was not significantly related to regular eye-tracking performance.", "target": "yes", "year": "2009", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "24622801", "question": "Does implant coating with antibacterial-loaded hydrogel reduce bacterial colonization and biofilm formation in vitro?", "context": "Implant-related infections represent one of the most severe complications in orthopaedics. A fast-resorbable, antibacterial-loaded hydrogel may reduce or prevent bacterial colonization and biofilm formation of implanted biomaterials.QUESTIONS/\n\nWe asked: (1) Is a fast-resorbable hydrogel able to deliver antibacterial compounds in vitro? (2) Can a hydrogel (alone or antibacterial-loaded) coating on implants reduce bacterial colonization? And (3) is intraoperative coating feasible and resistant to press-fit implant insertion?\n\nWe tested the ability of Disposable Antibacterial Coating (DAC) hydrogel (Novagenit Srl, Mezzolombardo, Italy) to deliver antibacterial agents using spectrophotometry and a microbiologic assay. Antibacterial and antibiofilm activity were determined by broth microdilution and a crystal violet assay, respectively. Coating resistance to press-fit insertion was tested in rabbit tibias and human femurs.\n\nComplete release of all tested antibacterial compounds was observed in less than 96 hours. Bactericidal and antibiofilm effect of DAC hydrogel in combination with various antibacterials was shown in vitro. Approximately 80% of the hydrogel coating was retrieved on the implant after press-fit insertion.", "target": "yes", "year": "2014", "labels": ["BACKGROUND", "PURPOSES", "METHODS", "RESULTS"]}
{"id": "22537902", "question": "Colorectal cancer with synchronous liver metastases: does global management at the same centre improve results?", "context": "Synchronous liver metastases (SLM) occur in 20% of colorectal cancers (CRC). Resection of SLM and CLC can be undertaken at different centres (separate management, SM) or at the same centre (global management, GM).\n\nRetrospective study of SLM and CRC resections carried out during 01/2000 - 12/2006 by SM or GM, using a combined or delayed strategy.\n\nMorphologic characteristics and type of CRC and SLM resection were similar for the GM (n = 45) or SM (n = 66) groups. In patients with delayed liver resection (62 SM, 17 GM), chemotherapy prior to liver surgery was used in 92% and 38% of SM and GM patients (P<0.0001) and the median delay between procedures was 212 and 182 days, respectively (P = 0.04). First step of liver resection was more often performed during colorectal surgery in the GM group (62 vs. 6% for SM, P<0.0001) and the mean number of procedures (CRC+SLM) was lower (1.6 vs. 2.3, P = 0.003). Three-month mortality was 3% for GM and 0% for SM (n.s.). Overall survival rates were 67% and 51% for SM and GM at 3 years (n.s.), and 35 and 31% at 5 years (n.s.). Disease-free survival to 5 years was higher in SM patients (14% vs. 11%, P = 0.009).", "target": "no", "year": "2013", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "12913878", "question": "Locoregional opening of the rodent blood-brain barrier for paclitaxel using Nd:YAG laser-induced thermo therapy: a new concept of adjuvant glioma therapy?", "context": "Nd:YAG laser-induced thermo therapy (LITT) of rat brains is associated with blood-brain barrier (BBB) permeability changes. We address the question of whether LITT-induced locoregional disruption of the BBB could possibly allow a locoregional passage of chemotherapeutic agents into brain tissue to treat malignant glioma.STUDY DESIGN/\n\nCD Fischer rats were subject to LITT of the left forebrain. Disruption of the BBB was analyzed using Evans blue and immunohistochemistry (IH). Animals were perfused with paclitaxel, and high-pressure liquid chromatography (HPLC) was employed to analyze the content of paclitaxel in brain and plasma samples.\n\nLITT induces an opening of the BBB as demonstrated by locoregional extravasation of Evans blue, C3C, fibrinogen, and IgM. HPLC proved the passage of paclitaxel across the disrupted BBB.", "target": "yes", "year": "2003", "labels": ["BACKGROUND AND OBJECTIVES", "MATERIALS AND METHODS", "RESULTS"]}
{"id": "17551944", "question": "Doppler examination of uteroplacental circulation in early pregnancy: can it predict adverse outcome?", "context": "To determine whether spectral Doppler measurements obtained from bilateral uterine, arcuate, radial, and spiral arteries in early gestation correlate with adverse pregnancy outcome.\n\nOne hundred five pregnant women underwent transvaginal Doppler sonographic examination of uteroplacental circulation at 6-12 weeks' gestation. Resistance index (RI) and pulsatility index (PI) of bilateral uterine, arcuate, radial, and spiral arteries were measured. Diameters of gestational sac (GS) and yolk sac, crown-rump length (CRL), GS-CRL difference, and GS/CRL ratio were also recorded. Correlation was made with pregnancy outcome.\n\nSixteen women developed adverse pregnancy outcome. In these women, right uterine artery PI and RI were significantly higher than in women with normal obstetrical outcome. Spiral artery PI and RI values were also higher, but the difference was not statistically significant. GS-CRL difference, GS/CRL ratio, and yolk sac diameters were significantly lower in this group.", "target": "yes", "year": "2007", "labels": ["PURPOSE", "METHODS", "RESULTS"]}
{"id": "21172844", "question": "Does TDP-43 type confer a distinct pattern of atrophy in frontotemporal lobar degeneration?", "context": "To determine whether TDP-43 type is associated with distinct patterns of brain atrophy on MRI in subjects with pathologically confirmed frontotemporal lobar degeneration (FTLD).\n\nIn this case-control study, we identified all subjects with a pathologic diagnosis of FTLD with TDP-43 immunoreactive inclusions (FTLD-TDP) and at least one volumetric head MRI scan (n = 42). In each case we applied published criteria for subclassification of FTLD-TDP into FTLD-TDP types 1-3. Voxel-based morphometry was used to compare subjects with each of the different FTLD-TDP types to age- and gender-matched normal controls (n = 30). We also assessed different pathologic and genetic variants within, and across, the different types.\n\nTwenty-two subjects were classified as FTLD-TDP type 1, 9 as type 2, and 11 as type 3. We identified different patterns of atrophy across the types with type 1 showing frontotemporal and parietal atrophy, type 2 predominantly anterior temporal lobe atrophy, and type 3 predominantly posterior frontal atrophy. Within the FTLD-TDP type 1 group, those with a progranulin mutation had significantly more lateral temporal lobe atrophy than those without. All type 2 subjects were diagnosed with semantic dementia. Subjects with a pathologic diagnosis of FTLD with motor neuron degeneration had a similar pattern of atrophy, regardless of whether they were type 1 or type 3.", "target": "yes", "year": "2010", "labels": ["OBJECTIVE", "METHODS", "RESULTS"]}
{"id": "16956164", "question": "Do all ethnic groups in New Zealand exhibit socio-economic mortality gradients?", "context": "First, to establish whether a deprivation gradient in all-cause mortality exists for all ethnic groups within New Zealand; second, if such gradients do exist, whether their absolute slopes are the same; and third, if such gradients exist, what impact the unequal deprivation distributions of the different ethnic groups have on the observed ethnic inequalities in life expectancy at birth.\n\nAbridged lifetables for the period 1999-2003 were constructed using standard demographic methods for each of four ethnic groups (Asian, Pacific, Maori and European) by NZDep2001 quintile and sex. Gradients were estimated by fitting generalised linear models to the quintile-specific life expectancy estimates for each ethnic group (by sex). The contribution of variation in deprivation distributions to inter-ethnic inequalities in life expectancy was estimated by re-weighting the quintile-specific mortality rates for each ethnic group using weights derived from the European deprivation distribution and recalculating the lifetable.\n\nAll four ethnic groups exhibit deprivation gradients in all-cause mortality (life expectancy). Maori show the steepest gradients, with slopes approximately 25% steeper than those of Europeans for both males and females. By contrast, gradients among Asian and Pacific peoples are shallower than those of their European counterparts.", "target": "yes", "year": "2006", "labels": ["OBJECTIVES", "METHOD", "RESULTS"]}
{"id": "19406119", "question": "Does telmisartan prevent hepatic fibrosis in rats with alloxan-induced diabetes?", "context": "This study evaluated the effect of telmisartan on the livers of diabetic rats and also aimed to determine the hepatic distribution and role of transforming growth factor beta (TGF-beta) in diabetes-related hepatic degeneration while taking into account the possible protective effects of telmisartan.\n\nFifteen adult male rats were used and divided into three groups: the non-diabetic healthy group, alloxan-induced diabetic control group, and the alloxan-induced diabetic telmisartan group. The non-diabetic healthy group and the diabetic control group were exposed to saline for 30 days, while the group treated with diabetic drugs was orally administered telmisartan for 30 days (10 mg/kg/day). At the end of the experiment, the rats were sacrificed and the livers were dissected and transferred into the fixation solution. The livers were then evaluated using stereological and histopathological methods.\n\nOur study of the numerical density of hepatocytes shows a significant difference between the diabetic control group and diabetic rats treated with telmisartan. Immunohistochemical staining for TGF-beta in liver sections of the diabetic rats treated with telmisartan showed no immunoreactivity. The diabetic control group was determined to be strongly immunoreactive to TGF-beta.", "target": "yes", "year": "2009", "labels": ["AIMS", "METHODS", "RESULTS"]}
{"id": "16909975", "question": "Can dose reduction to one parotid gland prevent xerostomia?", "context": "Dryness of the mouth is one of the most distressing chronic toxicities of radiation therapy in head and neck cancers. In this study, parotid function was assessed in patients with locally advanced head and neck cancers undergoing intensity-modulated radiotherapy (IMRT) with or without chemotherapy. Parotid function was assessed with the help of a questionnaire and parotid scintigraphy, especially with regards to unilateral sparing of the parotid gland.\n\nIn total, 19 patients were treated with compensator-based IMRT between February 2003 and March 2004. The dose to the clinical target volume ranged between 66 and 70 Gy in 30-35 fractions to 95% of the isodose volume. Ipsilateral high-risk neck nodes received an average dose of 60 Gy and the contralateral low-risk neck received a dose of 54-56 Gy. Eight of 19 patients also received concomitant chemotherapy.\n\nSubjective toxicity to the parotid glands was assessed with the help of a questionnaire at 0, 3 and 6 months and objective toxicity was assessed with parotid scintigraphy at 0 and 3 months. The mean dose to the ipsilateral parotid gland ranged from 19.5 to 52.8 Gy (mean 33.14 Gy) and the mean dose to the contralateral gland was 11.1-46.6 Gy (mean 26.85 Gy). At a median follow-up of 13 months, 9/19 patients had no symptoms of dryness of the mouth (grade I), 8/19 had mild dryness of the mouth (grade II) and only 2/19 had grade III xerostomia, although the parotid gland could only be spared on one side in most of the patients.", "target": "yes", "year": "2006", "labels": ["AIMS", "MATERIALS AND METHODS", "RESULTS"]}
{"id": "12769830", "question": "Should tumor depth be included in prognostication of soft tissue sarcoma?", "context": "Most staging systems for soft tissue sarcoma are based on histologic malignancy-grade, tumor size and tumor depth. These factors are generally dichotomized, size at 5 cm. We believe it is unlikely that tumor depth per se should influence a tumor's metastatic capability. Therefore we hypothesized that the unfavourable prognostic importance of depth could be explained by the close association between size and depth, deep-seated tumors on average being larger than the superficial ones. When tumor size is dichotomized, this effect should be most pronounced in the large size (>5 cm) group in which the size span is larger.\n\nWe analyzed the associations between tumor size and depth and the prognostic importance of grade, size and depth in a population-based series of 490 adult patients with soft tissue sarcoma of the extremity or trunk wall with complete, 4.5 years minimum, follow-up.\n\nMultivariate analysis showed no major prognostic effect of tumor depth when grade and size were taken into account. The mean size of small tumors was the same whether superficial or deep but the mean size of large and deep-seated tumors were one third larger than that of large but superficial tumors. Tumor depth influenced the prognosis in the subset of high-grade and large tumors. In this subset deep-seated tumors had poorer survival rate than superficial tumors, which could be explained by the larger mean size of the deep-seated tumors.", "target": "no", "year": "2003", "labels": ["BACKGROUND", "METHODS", "RESULTS"]}
{"id": "22237146", "question": "Can serum be used for analyzing the EGFR mutation status in patients with advanced non-small cell lung cancer?", "context": "Epidermal growth factor receptor (EGFR) mutations as prognostic or predictive marker in patients with non-small cell lung cancer (NSCLC) have been used widely. However, it may be difficult to get tumor tissue for analyzing the status of EGFR mutation status in large proportion of patients with advanced disease.\n\nWe obtained pairs of tumor and serum samples from 57 patients with advanced NSCLC, between March 2006 and January 2009. EGFR mutation status from tumor samples was analyzed by genomic polymerase chain reaction and direct sequence and EGFR mutation status from serum samples was determined by the peptide nucleic acid locked nucleic acid polymerase chain reaction clamp.\n\nEGFR mutations were detected in the serum samples of 11 patients and in the tumor samples of 12 patients. EGFR mutation status in the serum and tumor samples was consistent in 50 of the 57 pairs (87.7%). There was a high correlation between the mutations detected in serum sample and the mutations detected in the matched tumor sample (correlation index 0.62; P<0.001). Twenty-two of 57 patients (38.5%) received EGFR-tyrosine kinase inhibitors as any line therapy. The response for EGFR-tyrosine kinase inhibitors was significantly associated with EGFR mutations in both tumor samples and serum samples (P<0.05). There was no significant differences in overall survival according to the status of EGFR mutations in both serum and tumor samples (P>0.05).", "target": "yes", "year": "2013", "labels": ["BACKGROUND", "PATIENTS AND METHODS", "RESULTS"]}
{"id": "25079920", "question": "Do parents recall and understand children's weight status information after BMI screening?", "context": "As parents of young children are often unaware their child is overweight, screening provides the opportunity to inform parents and provide the impetus for behaviour change. We aimed to determine if parents could recall and understand the information they received about their overweight child after weight screening.\n\nRandomised controlled trial of different methods of feedback.\n\nParticipants were recruited through primary and secondary care but appointments took place at a University research clinic.\n\n1093 children aged 4-8 years were screened. Only overweight children (n=271, 24.7%) are included in this study. Parents of overweight children were randomised to receive feedback regarding their child's weight using best practice care (BPC) or motivational interviewing (MI) at face-to-face interviews typically lasting 20-40 min. 244 (90%) parents participated in a follow-up interview 2 weeks later to assess recall and understanding of information from the feedback session.\n\nInterviews were audio-taped and transcribed verbatim before coding for amount and accuracy of recall. Scores were calculated for total recall and sub-categories of interest.\n\nOverall, 39% of the information was recalled (mean score 6.3 from possible score of 16). Parents given feedback via BPC recalled more than those in the MI group (difference in total score 0.48; 95% CI 0.05 to 0.92). Although 94% of parents were able to correctly recall their child's weight status, fewer than 10 parents could accurately describe what the measurements meant. Maternal education (0.81; 0.25 to 1.37) and parental ratings of how useful they found the information (0.19; 0.04 to 0.35) were significant predictors of recall score in multivariate analyses.", "target": "maybe", "year": "2014", "labels": ["OBJECTIVES", "DESIGN", "SETTING", "PARTICIPANTS AND INTERVENTION", "PRIMARY AND SECONDARY OUTCOME MEASURES", "RESULTS"]}
{"id": "10966337", "question": "A short stay or 23-hour ward in a general and academic children's hospital: are they effective?", "context": "We evaluated the usefulness of a short stay or 23-hour ward in a pediatric unit of a large teaching hospital, Westmead Hospital, and an academic Children's hospital, The New Children's Hospital, to determine if they are a useful addition to the emergency service.\n\nThis is a descriptive comparison of prospectively collected data on all children admitted to the short stay ward at Westmead Hospital (WH) during 1994 and the short stay ward at the New Children's Hospital (NCH) during 1997-98. These hospitals service an identical demographic area with the latter (NCH) a tertiary referral center. The following outcome measures were used: length of stay, appropriateness of stay, rate of admission to an in-hospital bed, and rate of unscheduled visits within 72 hours of discharge. Adverse events were reported and patient follow-up was attempted at 48 hours after discharge in all cases.\n\nThe short stay ward accounted for 10.3% (Westmead Hospital) and 14.7% (New Children's Hospital) of admissions, with 56% medical in nature, 30% surgical, and the remainder procedural or psychological. Admission patterns were similar, with asthma, gastroenteritis, convulsion, pneumonia, and simple surgical conditions accounting for most short stay ward admissions. The short stay ward increased hospital efficiency with an average length of stay of 17.5 hours (Westmead Hospital) compared to 20.5 hours (New Children's Hospital). The users of the short stay ward were children of young age less than 2 years, with stay greater than 23 hours reported in only 1% of all admissions to the short stay ward. The rate of patient admission to an in-hospital bed was low, (4% [Westmead Hospital] compared to 6% [New Children's Hospital]), with the number of unscheduled visits within 72 hours of short stay ward discharge less than 1%. There were no adverse events reported at either short stay ward, with parental satisfaction high. The short stay ward was developed through reallocation of resources from within the hospital to the short stay ward. This resulted in estimated savings of $1/2 million (Westmead Hospital) to $2.3 million (New Children's Hospital) to the hospital, due to more efficient bed usage.", "target": "yes", "year": "2000", "labels": ["OBJECTIVE", "METHODS", "RESULTS"]}
{"id": "23495128", "question": "The colour of pain: can patients use colour to describe osteoarthritis pain?", "context": "The aim of the present study was to explore patients' views on the acceptability and feasibility of using colour to describe osteoarthritis (OA) pain, and whether colour could be used to communicate pain to healthcare professionals.\n\nSix group interviews were conducted with 17 patients with knee OA. Discussion topics included first impressions about using colour to describe pain, whether participants could associate their pain with colour, how colours related to changes to intensity and different pain qualities, and whether they could envisage using colour to describe pain to healthcare professionals.\n\nThe group interviews indicated that, although the idea of using colour was generally acceptable, it did not suit all participants as a way of describing their pain. The majority of participants chose red to describe high-intensity pain; the reasons given were because red symbolized inflammation, fire, anger and the stop signal in a traffic light system. Colours used to describe the absence of pain were chosen because of their association with positive emotional feelings, such as purity, calmness and happiness. A range of colours was chosen to represent changes in pain intensity. Aching pain was consistently identified as being associated with colours such as grey or black, whereas sharp pain was described using a wider selection of colours. The majority of participants thought that they would be able to use colour to describe their pain to healthcare professionals, although issues around the interpretability and standardization of colour were raised.", "target": "yes", "year": "2014", "labels": ["OBJECTIVE", "METHODS", "RESULTS"]}
{"id": "10577397", "question": "Is a pressor necessary during aortic perfusion and oxygenation therapy of cardiac arrest?", "context": "Occlusion of the descending aorta and infusion of oxygenated ultrapurified polymerized bovine hemoglobin may improve the efficacy of advanced cardiac life support (ACLS). Because selective aortic perfusion and oxygenation (SAPO) directly increases coronary perfusion pressure, exogenous epinephrine may not be required. The purpose of this study was to determine whether exogenous epinephrine is necessary during SAPO by comparing the rate of return of spontaneous circulation and aortic and coronary perfusion pressures during ACLS-SAPO in animals treated with either intra-aortic epinephrine or saline solution.\n\nA prospective, randomized, interventional before-after trial with a canine model of ventricular fibrillation cardiac arrest and ACLS based on external chest compression was performed. The ECG, right atrial, aortic arch, and esophageal pulse pressures were measured continuously. A descending aortic occlusion balloon catheter was placed through the femoral artery. Ventricular fibrillation was induced, and no therapy was given during the 10-minute arrest time. Basic life support was then initiated and normalized by standardization of esophageal pulse pressure and central aortic blood gases. After 3 minutes of basic life support, the aortic occlusion balloon was inflated, and 0.01 mg/kg epinephrine or saline solution was administered through the aortic catheter followed by 450 mL of ultrapurified polymerized bovine hemoglobin over 2 minutes. Defibrillation was then attempted. The outcomes and changes in intravascular pressures were compared.\n\nAortic pressures were higher during infusions in animals treated with epinephrine. During infusion, the mean aortic relaxation pressure increased by 58+/-5 mm Hg in animals that had received epinephrine versus 20+/-11 mm Hg in those that had received saline placebo. The coronary perfusion pressure during infusion increased by 52+/-8 mm Hg in animals that had received epinephrine versus 26+/-10 mm Hg in those that had received saline. Only 2 of 7 animals in the placebo group had return of spontaneous circulation versus 7 of 8 in the epinephrine group.", "target": "yes", "year": "1999", "labels": ["STUDY OBJECTIVE", "METHODS", "RESULTS"]}
{"id": "16195477", "question": "Production of chemokines by perivascular adipose tissue: a role in the pathogenesis of atherosclerosis?", "context": "Obesity is associated with an increased risk for cardiovascular disease. Although it is known that white adipose tissue (WAT) produces numerous proinflammatory and proatherogenic cytokines and chemokines, it is unclear whether adipose-derived chemotactic signals affect the chronic inflammation in atherosclerosis.\n\nHistological examination showed that perivascular WAT (pWAT) is in close proximity to vascular walls, particularly at sites that have a tendency to develop atherosclerosis. In rodents, the amount of pWAT is markedly increased by a high-fat diet. At a functional level, supernatant from subcutaneous and pWAT strongly induced the chemotaxis of peripheral blood leukocytes. The migration of granulocytes and monocytes was mostly mediated by interleukin-8 and monocyte chemoattractant protein-1, respectively, whereas both chemokines contributed to the migration of activated T cells. Moreover, pWAT produces these chemokines, as shown by immunohistochemistry and by explant culture. The accumulation of macrophages and T cells at the interface between pWAT and the adventitia of human atherosclerotic aortas may reflect this prochemotactic activity of pWAT.", "target": "yes", "year": "2005", "labels": ["OBJECTIVE", "METHODS AND RESULTS"]}
{"id": "18719011", "question": "Do overweight children necessarily make overweight adults?", "context": "To compare growth curves of body mass index from children to adolescents, and then to young adults, in Japanese girls and women in birth cohorts born from 1930 to 1999.\n\nRetrospective repeated cross sectional annual nationwide surveys (national nutrition survey, Japan) carried out from 1948 to 2005.\n\nJapan.\n\n76,635 females from 1 to 25 years of age.\n\nBody mass index.\n\nGenerally, body mass index decreased in preschool children (2-5 years), increased in children (6-12 years) and adolescents (13-18 years), and slightly decreased in young adults (19-25 years) in these Japanese females. However, the curves differed among birth cohorts. More recent cohorts were more overweight as children but thinner as young women. The increments in body mass index in early childhood were larger in more recent cohorts than in older cohorts. However, the increments in body mass index in adolescents were smaller and the decrease in body mass index in young adults started earlier, with lower peak values in more recent cohorts than in older cohorts. The decrements in body mass index in young adults were similar in all birth cohorts.", "target": "no", "year": "2008", "labels": ["OBJECTIVE", "DESIGN", "SETTING", "PARTICIPANTS", "MAIN OUTCOME MEASURE", "RESULTS"]}
{"id": "23389866", "question": "Chemoradiotherapy in the management of locally advanced squamous cell carcinoma esophagus: is surgical resection required?", "context": "The present study aims to evaluate benefit of adding surgery to chemoradiotherapy alone in management of carcinoma esophagus.\n\nWe retrospectively analyzed 45 eligible patients of squamous cell esophageal carcinoma which were enrolled from February 2008 to April 2009. All patients were treated with chemoradiotherapy (50.40 Gy with 40 mg/m(2) of weekly cisplatin). Tumor response was assessed after 6 weeks of treatment. Patients with resectable disease were subjected to surgical resection (arm A) and remaining was kept on regular clinical follow-up (arm B). Overall survival (OS) was selected as the primary endpoint. The secondary end points were disease-free survival (DFS) and clinical toxicities.\n\nMedian follow-up was 13.6 months. Pathological complete response was seen in 60.9 % patients in arm A. In arm B, 77.3 % patients attained radiological complete response (p = 0.194). The median OS was 16.4 and 19.1 months (p = 0.388) and median DFS was 5.8 and 4.1 months (p = 0.347) in arm A and B, respectively. The 2-year survival probability was 39.1 and 36.4 % (p = 0.387) in arm A and B, respectively. The recurrence probability was 56.5 % (SE = 5.6 %) and 45.5 % (SE = 4.2 %) (p = 0.328) in arm A and B, respectively. The probability of loco regional recurrence was more in arm B than in arm A (p = 0.002).", "target": "no", "year": "2013", "labels": ["PURPOSE", "METHODS", "RESULTS"]}
