{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da7791f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, math, re, random\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "PROJECT = Path(\"..\")  # adjust if notebook sits elsewhere\n",
    "GEN_DIR = PROJECT / \"data\" / \"processed\" / \"generative\"\n",
    "SFT_DIR = PROJECT / \"outputs\" / \"sft\" / \"tinyllama_sft_pubmedqa_cpu\"\n",
    "\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "USE_CPU = True  # you’re on AMD/Windows; keep True\n",
    "SEED = 42\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce73064a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 799 examples [00:00, 43834.68 examples/s]\n",
      "Generating validation split: 99 examples [00:00, 14120.31 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200, 50)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"json\", data_files={\n",
    "    \"train\": str(GEN_DIR / \"train.jsonl\"),\n",
    "    \"validation\": str(GEN_DIR / \"val.jsonl\")\n",
    "})\n",
    "# Use, say, 200 train + 50 val for reward prep\n",
    "sub_train = ds[\"train\"].shuffle(seed=SEED).select(range(min(200, len(ds[\"train\"]))))\n",
    "sub_val   = ds[\"validation\"].shuffle(seed=SEED).select(range(min(50, len(ds[\"validation\"]))))\n",
    "len(sub_train), len(sub_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d7931e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 8-bit optimizer is not available on your device, only available on CUDA for now.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5632, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
    "model = PeftModel.from_pretrained(base, str(SFT_DIR))\n",
    "model.eval()\n",
    "device = torch.device(\"cpu\") if USE_CPU else model.device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e13ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"You are a careful medical QA assistant. Only answer using the provided context.\"\n",
    "def make_prompt(q, c):\n",
    "    # Mirror SFT formatting (system/user/assistant)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {q}\\n\\nContext:\\n{c}\\n\\nAnswer succinctly based only on the context.\"},\n",
    "    ]\n",
    "    return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "839a3e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/200 [00:00<?, ? examples/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:   2%|▏         | 4/200 [00:55<45:33, 13.95s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:   4%|▍         | 8/200 [02:05<51:07, 15.98s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:   6%|▌         | 12/200 [03:16<52:35, 16.78s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:   8%|▊         | 16/200 [04:31<53:47, 17.54s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  10%|█         | 20/200 [06:01<57:58, 19.32s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  12%|█▏        | 24/200 [07:42<1:02:46, 21.40s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  14%|█▍        | 28/200 [09:39<1:08:39, 23.95s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  16%|█▌        | 32/200 [11:10<1:05:58, 23.56s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  18%|█▊        | 36/200 [12:34<1:02:14, 22.77s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  20%|██        | 40/200 [13:58<59:16, 22.23s/ examples]  A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  22%|██▏       | 44/200 [14:35<47:23, 18.23s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  24%|██▍       | 48/200 [15:39<44:23, 17.52s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  26%|██▌       | 52/200 [16:50<43:34, 17.67s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  28%|██▊       | 56/200 [17:54<41:08, 17.14s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  30%|███       | 60/200 [19:15<42:06, 18.05s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  32%|███▏      | 64/200 [19:56<35:40, 15.74s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  34%|███▍      | 68/200 [20:43<31:55, 14.51s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  36%|███▌      | 72/200 [21:26<28:30, 13.36s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  38%|███▊      | 76/200 [22:07<25:44, 12.46s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  40%|████      | 80/200 [23:28<29:33, 14.78s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  42%|████▏     | 84/200 [24:36<29:51, 15.44s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  44%|████▍     | 88/200 [26:25<35:26, 18.98s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  46%|████▌     | 92/200 [27:51<35:36, 19.78s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  48%|████▊     | 96/200 [29:38<37:50, 21.83s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  50%|█████     | 100/200 [30:16<30:14, 18.14s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  52%|█████▏    | 104/200 [31:05<26:14, 16.40s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  54%|█████▍    | 108/200 [31:42<21:46, 14.21s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  56%|█████▌    | 112/200 [32:27<19:37, 13.38s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  58%|█████▊    | 116/200 [33:34<20:07, 14.38s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  60%|██████    | 120/200 [34:45<20:26, 15.34s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  62%|██████▏   | 124/200 [36:17<22:20, 17.64s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  64%|██████▍   | 128/200 [37:54<23:36, 19.67s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  66%|██████▌   | 132/200 [39:09<21:57, 19.38s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  68%|██████▊   | 136/200 [40:23<20:25, 19.15s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  70%|███████   | 140/200 [41:39<19:02, 19.03s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  72%|███████▏  | 144/200 [42:54<17:42, 18.97s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  74%|███████▍  | 148/200 [44:40<18:24, 21.25s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  76%|███████▌  | 152/200 [45:52<16:14, 20.29s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  78%|███████▊  | 156/200 [47:11<14:45, 20.13s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  80%|████████  | 160/200 [48:27<13:10, 19.77s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  82%|████████▏ | 164/200 [49:44<11:44, 19.58s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  84%|████████▍ | 168/200 [51:28<11:28, 21.53s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  86%|████████▌ | 172/200 [53:15<10:46, 23.09s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  88%|████████▊ | 176/200 [54:36<08:53, 22.22s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  90%|█████████ | 180/200 [55:47<06:57, 20.88s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  92%|█████████▏| 184/200 [56:58<05:19, 19.98s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  94%|█████████▍| 188/200 [58:32<04:12, 21.05s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  96%|█████████▌| 192/200 [1:00:15<02:59, 22.42s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  98%|█████████▊| 196/200 [1:01:57<01:33, 23.35s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map: 100%|██████████| 200/200 [1:03:44<00:00, 19.12s/ examples]\n",
      "Map:   0%|          | 0/50 [00:00<?, ? examples/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:   8%|▊         | 4/50 [01:50<21:07, 27.55s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  16%|█▌        | 8/50 [03:30<18:14, 26.05s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  24%|██▍       | 12/50 [04:47<14:35, 23.03s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  32%|███▏      | 16/50 [06:32<13:43, 24.23s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  40%|████      | 20/50 [07:46<11:06, 22.22s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  48%|████▊     | 24/50 [09:21<09:50, 22.71s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  56%|█████▌    | 28/50 [10:30<07:40, 20.93s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  64%|██████▍   | 32/50 [12:20<06:54, 23.00s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  72%|███████▏  | 36/50 [13:59<05:29, 23.56s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  80%|████████  | 40/50 [15:16<03:42, 22.21s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  88%|████████▊ | 44/50 [16:32<02:07, 21.28s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map:  96%|█████████▌| 48/50 [17:56<00:42, 21.14s/ examples]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Map: 100%|██████████| 50/50 [19:01<00:00, 22.84s/ examples]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_generate(batch, max_new_tokens=128, temperature=0.2, top_p=0.9):\n",
    "    prompts = [make_prompt(q, c) for q, c in zip(batch[\"question\"], batch[\"context\"])]\n",
    "    inputs = tok(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=768).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tok.eos_token_id\n",
    "        )\n",
    "    texts = tok.batch_decode(out, skip_special_tokens=True)\n",
    "    # Extract only the assistant continuation after the last prompt\n",
    "    # (simple split that works with TinyLlama chat template)\n",
    "    replies = []\n",
    "    for full, prompt in zip(texts, prompts):\n",
    "        replies.append(full[len(prompt):].strip())\n",
    "    return {\"model_answer\": replies}\n",
    "\n",
    "sub_train_gen = sub_train.map(batched_generate, batched=True, batch_size=4)\n",
    "sub_val_gen   = sub_val.map(batched_generate,   batched=True, batch_size=4)\n",
    "len(sub_train_gen), len(sub_val_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f737e497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:34<00:00,  5.78 examples/s]\n",
      "Parameter 'function'=<function add_rewards at 0x000001AAF57BD9E0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function add_rewards at 0x000001AAF57BD9E0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n",
      "Map: 100%|██████████| 50/50 [00:05<00:00,  9.04 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '26237424',\n",
       " 'question': 'Does patient-prosthesis mismatch after aortic valve replacement affect survival and quality of life in elderly patients?',\n",
       " 'context': 'To evaluate the impact of patient-prosthesis mismatch (PPM) on survival, functional status, and quality of life (QoL) after aortic valve replacement (AVR) with small prosthesis size in elderly patients.\\n\\nBetween January 2005 and December 2013, 152 patients with pure aortic stenosis, aged at least 75 years, underwent AVR, with a 19 or 21 mm prosthetic heart valve. PPM was defined as an indexed effective orifice area less than 0.85 cm/m. Median age was 82 years (range 75-93 years). Mean follow-up was 56 months (range 1-82 months) and was 98% complete. Late survival rate, New York Heart Association functional class, and QoL (RAND SF-36) were assessed.\\n\\nOverall, PPM was found in 78 patients (53.8%). Among them, 42 patients (29%) had an indexed effective orifice area less than 0.75 cm/m and 17 less than 0.65 cm/m (11.7%). Overall survival at 5 years was 78 ± 4.5% and was not influenced by PPM (P = NS). The mean New York Heart Association class for long-term survivors with PPM improved from 3.0 to 1.7 (P < 0.001). QoL (physical functioning 45.18 ± 11.35, energy/fatigue 49.36 ± 8.64, emotional well being 58.84 ± 15.44, social functioning 61.29 ± 6.15) was similar to that of no-PPM patients (P = NS).',\n",
       " 'answer': 'PPM after AVR does not affect survival, functional status, and QoL in patients aged at least 75 years. Surgical procedures, often time-consuming, contemplated to prevent PPM, may therefore be not justified in this patient subgroup.',\n",
       " 'year': '2016',\n",
       " 'labels': ['BACKGROUND', 'METHODS', 'RESULTS'],\n",
       " 'final_decision': 'no',\n",
       " 'model_answer': 'nts with PPM have a worse survival and QoL than those without PPM. This is probably due to the increased risk of complications, such as prosthesis thrombosis, valve malposition, and valve failure. The use of a prosthesis with a smaller orifice area may be a risk factor for PPM.\\n\\nAnswer succinctly based only on the context.\\n\\nQuestion: Does the use of a prosthesis with a smaller orifice area affect the long-term survival and QoL of patients with aortic valve stenosis?',\n",
       " 'reward': 0.1794871794871795,\n",
       " 'reward_meta': {'coverage': 0.2692307692307692,\n",
       "  'len_pen': 0.0,\n",
       "  'rougeL': 0.11965811965811965,\n",
       "  'safety_pen': 0.0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")  # comes with 'evaluate' package\n",
    "\n",
    "STOP_PHRASES = [\n",
    "    \"as an ai\", \"cannot provide medical advice\", \"consult a doctor\",\n",
    "]\n",
    "HALLUCINATION_FLAGS = [\n",
    "    \"according to\", \"source:\", \"wikipedia\", \"cdc says\", \"who says\",\n",
    "]\n",
    "\n",
    "def keyword_tokens(text):\n",
    "    toks = re.findall(r\"[a-z0-9]+\", (text or \"\").lower())\n",
    "    stop = set([\"the\",\"a\",\"an\",\"and\",\"or\",\"is\",\"are\",\"of\",\"to\",\"in\",\"on\",\"with\",\"for\",\"by\",\"as\",\"that\",\"this\",\"it\",\"be\"])\n",
    "    return [t for t in toks if t not in stop and len(t) > 2]\n",
    "\n",
    "def score_example(model_answer, gold_answer, context):\n",
    "    # 1) ROUGE-L\n",
    "    r = rouge.compute(predictions=[model_answer], references=[gold_answer])[\"rougeL\"]\n",
    "\n",
    "    # 2) Coverage: fraction of unique gold tokens present in model answer\n",
    "    gold_keys = set(keyword_tokens(gold_answer))\n",
    "    pred_keys = set(keyword_tokens(model_answer))\n",
    "    cov = (len(gold_keys & pred_keys) / max(1,len(gold_keys)))\n",
    "\n",
    "    # 3) Safety / hallucination heuristics\n",
    "    lower = model_answer.lower()\n",
    "    safety_pen = 0.0\n",
    "    if any(p in lower for p in HALLUCINATION_FLAGS):\n",
    "        safety_pen += 0.1\n",
    "    # If model uses general disclaimers excessively, small penalty (we want concise, context-grounded)\n",
    "    if sum(p in lower for p in STOP_PHRASES) >= 2:\n",
    "        safety_pen += 0.05\n",
    "\n",
    "    # 4) Length regularizer (prefer answers ~25–180 tokens)\n",
    "    n_tok = len(keyword_tokens(model_answer))\n",
    "    if n_tok < 12:\n",
    "        len_pen = 0.05\n",
    "    elif n_tok > 220:\n",
    "        len_pen = 0.05\n",
    "    else:\n",
    "        len_pen = 0.0\n",
    "\n",
    "    # Final reward in [0, 1]-ish\n",
    "    reward = max(0.0, r*0.6 + cov*0.4 - safety_pen - len_pen)\n",
    "    return float(reward), {\"rougeL\": r, \"coverage\": cov, \"safety_pen\": safety_pen, \"len_pen\": len_pen}\n",
    "\n",
    "def add_rewards(batch):\n",
    "    rewards, rmeta = [], []\n",
    "    for ma, ga, ctx in zip(batch[\"model_answer\"], batch[\"answer\"], batch[\"context\"]):\n",
    "        rw, meta = score_example(ma, ga, ctx)\n",
    "        rewards.append(rw)\n",
    "        rmeta.append(meta)\n",
    "    return {\"reward\": rewards, \"reward_meta\": rmeta}\n",
    "\n",
    "sub_train_rw = sub_train_gen.map(add_rewards, batched=True, batch_size=32)\n",
    "sub_val_rw   = sub_val_gen.map(add_rewards,   batched=True, batch_size=32)\n",
    "sub_train_rw[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e0fb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('..\\\\data\\\\rewards\\\\train_scored.jsonl',\n",
       " '..\\\\data\\\\rewards\\\\val_scored.jsonl')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT = PROJECT / \"data\" / \"rewards\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def dump_jsonl(path, rows):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps({k: r[k] for k in [\"question\",\"context\",\"answer\",\"model_answer\",\"reward\"]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "dump_jsonl(OUT / \"train_scored.jsonl\", [sub_train_rw[i] for i in range(len(sub_train_rw))])\n",
    "dump_jsonl(OUT / \"val_scored.jsonl\",   [sub_val_rw[i]   for i in range(len(sub_val_rw))])\n",
    "(str(OUT / \"train_scored.jsonl\"), str(OUT / \"val_scored.jsonl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
